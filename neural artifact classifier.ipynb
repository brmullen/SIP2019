{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing slackNotify.py\n",
      "\t ERROR :  No module named 'slacker'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-361e95817ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"whitegrid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_codes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_credentials_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'brmullen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w9zNJSqbIzgDUYE9ET7R'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from waveletAnalysis import waveletAnalysis as wave\n",
    "import timecourseAnalysis as tca\n",
    "from hdf5manager import hdf5manager as h5\n",
    "from matplotlib import pyplot as plt\n",
    "from wholeBrainPCA import *\n",
    "import scipy\n",
    "import math\n",
    "from scipy import ndimage\n",
    "from skimage.measure import label, regionprops\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "plt.rc(\"font\", size=14)\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='brmullen', api_key='w9zNJSqbIzgDUYE9ET7R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/brian/Documents/data/Classifier/'\n",
    "file = '170804_02_ica.hdf5'\n",
    "\n",
    "h = h5(path + file)\n",
    "print(h.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped = h.load('flipped')\n",
    "tcourse = h.load('timecourses')\n",
    "\n",
    "#flipped the inverted timeseries\n",
    "tcourse = (np.multiply(tcourse.T, flipped)).T\n",
    "\n",
    "#load mask\n",
    "roimask = h.load('roimask')\n",
    "\n",
    "print(tcourse.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for key, val in data.items():\n",
    "#     print(key)\n",
    "#     print(val)\n",
    "# tcourse = h.load('ROI_timecourses')\n",
    "# roi = h.load('domain_ROIs')\n",
    "# # mask = h.load('roimask')\n",
    "# # roi[np.where(mask==0)] = np.nan\n",
    "\n",
    "# # mean = h.load('mean')\n",
    "# # tcourse += mean\n",
    "\n",
    "# # plt.plot(mean)\n",
    "# # plt.show()\n",
    "\n",
    "# plt.imshow(roi)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "plt.imshow(roimask)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for time information\n",
    "\n",
    "The following metrics should be taken to build the classifier:\n",
    "\n",
    "1. __Characterize the timeseries__\n",
    "<br>\n",
    "    a. Autocorrelation Lag1\n",
    "<br>\n",
    "    b. Standard Deviation\n",
    "<br>\n",
    "2. __Frequency analysis__: Range of frequencies from the longest region above the noise threshold\n",
    "<br>\n",
    "    a. Integrate the values above the noise cuttoff\n",
    "<br>\n",
    "    b. Find max SNR\n",
    "<br>\n",
    "    c. Average SNR\n",
    "<br>\n",
    "    d. Range of significance: Lowest/Highest frequencies\n",
    "<br>\n",
    "    e. Size of frequency span\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findContinBool(boolArray1D):\n",
    "    nestDict = {}\n",
    "    j, k = (-1, 0)\n",
    "    usek = True\n",
    "    for i in range(len(boolArray1D)):\n",
    "        if boolArray1D[i] and not boolArray1D[i-1]:\n",
    "            j += 1 \n",
    "            if j >= 1:\n",
    "                nestDict['region' + str(j-1)]['length'] = k\n",
    "            k = 0\n",
    "            nestDict['region' + str(j)] = {}\n",
    "            nestDict['region' + str(j)]['freq.index'] = [] \n",
    "            nestDict['region' + str(j)]['freq.index'].append(i)\n",
    "            k += 1\n",
    "        if boolArray1D[i] and boolArray1D[i-1]:\n",
    "            if i == 1:\n",
    "                j += 1\n",
    "                nestDict['region' + str(j)] = {}\n",
    "                nestDict['region' + str(j)]['freq.index'] = []\n",
    "                nestDict['region' + str(j)]['freq.index'].append(i-1)\n",
    "                nestDict['region' + str(j)]['freq.index'].append(i)\n",
    "            elif i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                k += 1\n",
    "                nestDict['region' + str(j)]['freq.index'].append(i)\n",
    "\n",
    "    if j != -1:\n",
    "        nestDict['region' + str(j)]['length'] = k\n",
    "    \n",
    "    return nestDict\n",
    "\n",
    "def findSig():\n",
    "    ratio = np.squeeze(w.gws/w.gws_sig)\n",
    "    index = (ratio > 1)\n",
    "    return ratio, index\n",
    "\n",
    "def approxIntegrate(index, freq, power, sigcutoff): \n",
    "    #power\n",
    "    diff = np.squeeze(power - sigcutoff) #integrate only significant area\n",
    "    #freq\n",
    "    if index[-1] != freq.shape[0]-1: # default to right sided estimation\n",
    "        offset = [x+1 for x in index]\n",
    "        binsz = freq[index] - freq[offset]\n",
    "    else:\n",
    "        offset = [x-1 for x in index]\n",
    "        binsz = freq[offset] - freq[index]\n",
    "        \n",
    "    return np.sum(binsz * diff[index])\n",
    "\n",
    "def temporalCharacterize(sigfreq, ratio, freq):\n",
    "    for k in sigfreq.keys():\n",
    "        sigfreq[k]['freq.maxsnr'] = np.nanmax(ratio[sigfreq[k]['freq.index']])\n",
    "        sigfreq[k]['freq.maxsnr.freq'] = np.nanargmax(ratio[sigfreq[k]['freq.index']])\n",
    "        sigfreq[k]['freq.avgsnr'] = np.nanmean(ratio[sigfreq[k]['freq.index']])\n",
    "        sigfreq[k]['freq.range.low'] = freq[sigfreq[k]['freq.index'][-1]] \n",
    "        sigfreq[k]['freq.range.high'] = freq[sigfreq[k]['freq.index'][0]]\n",
    "        if sigfreq[k]['freq.range.low'] and not sigfreq[k]['freq.range.high']:\n",
    "                sigfreq[k]['freq.range.high'] = 5\n",
    "        if sigfreq[k]['freq.range.high'] and not sigfreq[k]['freq.range.low']:\n",
    "                sigfreq[k]['freq.range.low'] = 0                \n",
    "        sigfreq[k]['freq.rangesz'] = (sigfreq[k]['freq.range.high'] - sigfreq[k]['freq.range.low'])\n",
    "        sigfreq[k]['freq.integrate'] = approxIntegrate(index = sigfreq[k]['freq.index'], freq = w.flambda, \n",
    "                                                  power = w.gws, sigcutoff = w.gws_sig)\n",
    "\n",
    "    return sigfreq\n",
    "\n",
    "def sortNestedDict(nestDict, sortkey = 'freq.rangesz'):\n",
    "    sortDict = []\n",
    "    for k in nestDict.keys(): sortDict.append(k)\n",
    "#     sortDict = sorted(sortDict, key=lambda x: (sigfreq[x]['length']), reverse =True)\n",
    "    sortDict = sorted(sortDict, key=lambda x: (nestDict[x][sortkey]), reverse =True)\n",
    "\n",
    "    return sortDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single index\n",
    "good_bad_index = [23]#, 150, 170]\n",
    "# plt.imshow(roi)\n",
    "# plt.show()\n",
    "fps = 10\n",
    "for index in good_bad_index:\n",
    "    print('Domian number :', index)\n",
    "    w = wave(data = tcourse[index],\n",
    "             fps = fps,\n",
    "             mother = 'MORLET',\n",
    "             param = 4,\n",
    "             siglvl = 0.95,\n",
    "             verbose = True,\n",
    "             plot=True)\n",
    "    w.globalWaveletSpectrum()\n",
    "    \n",
    "    ratio, index = findSig()\n",
    "    k = findContinBool(index)\n",
    "    k = temporalCharacterize(k, ratio, w.flambda)\n",
    "    l = sortNestedDict(k)\n",
    "    print(l)\n",
    "    print(k)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# autocorr = np.zeros((tcourse.shape[0])) * np.nan\n",
    "# maxfreq = np.zeros(tcourse.shape[0]) * np.nan\n",
    "# lowfreq = np.zeros(tcourse.shape[0]) * np.nan\n",
    "\n",
    "# for i, tc in enumerate(tcourse):\n",
    "#     if (i%100) == 0:\n",
    "#         print(i)\n",
    "#     autocorr[i] = tca.lagNAutoCorr(w.data, 1)\n",
    "#     w = wave(data = tc, fps = 10,verbose = False, plot = False)\n",
    "#     w.globalWaveletSpectrum()\n",
    "#     if hasattr(w, 'gws_lo_high_freq'):\n",
    "#         maxfreq[i] = w.gws_lo_high_freq[0][0]\n",
    "#         lowfreq[i] = w.gws_lo_high_freq[1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowfreq[np.where(lowfreq == 5)] = np.nan\n",
    "\n",
    "# plt.plot(autocorr[:,0], label = 'autocorr1')\n",
    "# plt.plot(autocorr[:,1], label = 'autocorr2')\n",
    "# plt.plot(autocorr[:,2], label = 'autocorr3')\n",
    "# plt.plot(maxfreq, label = 'lowfreq')\n",
    "# plt.plot(lowfreq, label = 'highfreq')\n",
    "\n",
    "# plt.scatter(np.arange(tcourse.shape[0]), -1/10*noise, marker='.', color = 'k', label = 'noise')\n",
    "# plt.xlim([0,300])\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for spatial analysis\n",
    "\n",
    "The following metrics should be taken to build the classifier:\n",
    "\n",
    "1. __Characterize the center of mass__\n",
    "<br>\n",
    "    a. Autocorrelation Lag1\n",
    "<br>\n",
    "    b. Standard Deviation\n",
    "<br>\n",
    "2. __Largest blob characterization__\n",
    "<br>\n",
    "    a. Percent 'mass' under the largest blob\n",
    "<br>\n",
    "    b. Percent of the masked region the largest blob composes\n",
    "<br>\n",
    "    c. Major/minor axis\n",
    "<br>\n",
    "    d. Orientation/ Angle of rotation for major axis\n",
    "<br>\n",
    "    e. Eccentricity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_id = 15\n",
    "#load mask\n",
    "roimask = h.load('roimask')\n",
    "#load eig_vec\n",
    "eig_vec = h.load('eig_vec')\n",
    "eigenbrain = rebuildEigenbrain(eig_vec, pc_id, roimask=roimask, eigb_shape=None, \n",
    "    maskind=None)\n",
    "\n",
    "flipped = h.load('flipped')\n",
    "\n",
    "thresh = h.load('thresholds')\n",
    "\n",
    "plt.imshow(eigenbrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def centerOfMass(eigenbrain, threshold=None, verbose=True, plot=True):\n",
    "    eigen = eigenbrain.copy()\n",
    "    if threshold is not None:\n",
    "        eigen[eigen < threshold] = np.nan\n",
    "    x, y = np.where(np.isnan(eigen)==False)\n",
    "\n",
    "    #total sum\n",
    "    totalmass = np.nansum(np.abs(eigen))\n",
    "    #weighted sum\n",
    "    sumrmx = 0\n",
    "    sumrmy = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        sumrmx += np.abs(eigenbrain[x[i],y[i]])*x[i]\n",
    "        sumrmy += np.abs(eigenbrain[x[i],y[i]])*y[i]\n",
    "\n",
    "#     plt.imshow(eigen)\n",
    "#     plt.show()\n",
    "    \n",
    "    comx = sumrmx/totalmass\n",
    "    comy = sumrmy/totalmass\n",
    "   \n",
    "    if verbose:\n",
    "        print('xcom: ', comx)\n",
    "        print('ycom: ', comy)\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(eigenbrain)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(comy, comx, color='w', marker='*' )\n",
    "        plt.show()\n",
    "\n",
    "    return comx, comy\n",
    "\n",
    "def xyProjectMax(eigenbrain, verbose=True, plot=True):\n",
    "    xmean = np.nanmean(eigenbrain, axis = 1)\n",
    "    xmax = np.nanargmax(xmean)\n",
    "    \n",
    "    ymean = np.nanmean(eigenbrain, axis = 0)\n",
    "    ymax = np.nanargmax(ymean)\n",
    "\n",
    "    if verbose:\n",
    "        print('xmax: ', xmax)\n",
    "        print('ymax: ', ymax)\n",
    "        \n",
    "    if plot:\n",
    "        plt.plot(ymean)\n",
    "        plt.plot(xmean)\n",
    "        plt.scatter([xmax,ymax],[0,0], color = 'r')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        plt.imshow(eigenbrain)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(ymax, xmax, color='w', marker='*' )\n",
    "        plt.show()\n",
    "\n",
    "    return xmax, ymax\n",
    "    \n",
    "def positionMaxIntensity(eigenbrain, verbose=True, plot=True):\n",
    "    amax = np.nanmax(eigenbrain)\n",
    "    xamax = np.where(eigenbrain == amax)[0]\n",
    "    yamax = np.where(eigenbrain == amax)[1]\n",
    "    if verbose:\n",
    "        print('x amax: ', xamax)\n",
    "        print('y amax: ', yamax)\n",
    "        \n",
    "    if plot:\n",
    "        plt.imshow(eigenbrain)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(yamax, xamax, color='w', marker='*' )\n",
    "        plt.show()\n",
    "    return xamax, yamax\n",
    "    \n",
    "def majorMinorAxis(x, y, scale=50, rotsum=False, plot=False):\n",
    "    \n",
    "    #Determine center of mass and planes of symmetry of a binary masked region.\n",
    "    xthresh = x - np.mean(x) #demean the data\n",
    "    ythresh = y - np.mean(y)\n",
    "    \n",
    "    coords = np.vstack([xthresh, ythresh])\n",
    "    \n",
    "    cov = np.cov(coords) #find the covarience\n",
    "    evals, evecs = np.linalg.eig(cov) #take the eigenvecotors of the covarince of the x, y coordinates\n",
    "    sort_indices = np.argsort(evals)[::-1] #sort the two eigenvectors (x, y  covarience)\n",
    "    evec1, evec2 = evecs[:, sort_indices]\n",
    "    x_v1, y_v1 = evec1  # Eigenvector with largest eigenvalue\n",
    "    x_v2, y_v2 = evec2\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(ythresh + np.mean(y), xthresh + np.mean(x), 'k.')\n",
    "        plt.plot([y_v1*-scale+ np.mean(y), y_v1*scale+ np.mean(y)],\n",
    "                 [x_v1*-scale+ np.mean(x), x_v1*scale+ np.mean(x)],\n",
    "                  color='blue')\n",
    "        plt.plot([y_v2*-scale+ np.mean(y), y_v2*scale+ np.mean(y)],\n",
    "                 [x_v2*-scale+ np.mean(x), x_v2*scale+ np.mean(x)],\n",
    "                  color='red')\n",
    "    #     ax.plot3D([x_v3*-scale+ np.mean(x), x_v3*scale+ np.mean(x)],\n",
    "    #              [y_v3*-scale+ np.mean(y), y_v3*scale+ np.mean(y)],\n",
    "    #              [z_v3*-scale+ np.mean(z), z_v3*scale+ np.mean(z)], color='green')\n",
    "    #     plt.axis('equal')\n",
    "    #     plt.gca().invert_yaxis()  # Match the image system with origin at top left\n",
    "    #     ax.view_init(60, 35)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show\n",
    "    \n",
    "    if rotsum:\n",
    "    # rotate the image based on major axis, project along the major and minor axis\n",
    "        angle = math.degrees(math.atan(y_v1/x_v1))\n",
    "\n",
    "        rot = rotate(eigenbrain,-angle, resize = True, cval=np.nan )\n",
    "\n",
    "        rotx = np.nanmean(test, axis =0)\n",
    "        roty = np.nanmean(test, axis =1)\n",
    "        \n",
    "        if plot:\n",
    "            plt.imshow(test)\n",
    "            plt.show()\n",
    "            \n",
    "            plt.plot(rotx, label='rotx')\n",
    "            plt.plot(roty, label = 'roty')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    major = [[x_v1*-scale + np.mean(x),x_v1*scale + np.mean(x)],\n",
    "             [y_v1*-scale + np.mean(y),y_v1*scale + np.mean(y)]]\n",
    "\n",
    "    minor = [[x_v2*-scale + np.mean(x),x_v2*scale + np.mean(x)],\n",
    "             [y_v2*-scale + np.mean(y),y_v2*scale + np.mean(y)]]\n",
    "    \n",
    "    if rotsum:\n",
    "        return major, minor, rotx, roty\n",
    "    else:\n",
    "        return major, minor\n",
    "\n",
    "def spatialCharacterize(eigenbrain, pc, threshold, verbose = False, plot = False):\n",
    "    eigen = eigenbrain.copy()\n",
    "#     eigen[eigen == np.nan] = 0\n",
    "    eigen[eigen < threshold] = np.nan  \n",
    "    x, y = np.where(np.isnan(eigen)==False)\n",
    "    image = np.zeros_like(eigenbrain)\n",
    "    image[x,y] = 1\n",
    "    image = scipy.ndimage.median_filter(image, size=5)\n",
    "    label_img = label(image)\n",
    "    regions = regionprops(label_image)\n",
    "    totalmass = np.nansum(np.abs(eigenbrain))\n",
    "    \n",
    "    domregion = {}\n",
    "    \n",
    "    for i, props in enumerate(regions):\n",
    "        domregion['region' + str(i)] = {}\n",
    "        regcoord = props.coords\n",
    "        intensity = np.zeros_like(regcoord[:,0]).astype('float16')\n",
    "        for j, coord in enumerate(regcoord):\n",
    "            intensity[j] = eigenbrain[coord[0], coord[1]]\n",
    "        \n",
    "        if plot:\n",
    "            plt.scatter(y, x, color = (0,0,0,0.05), marker='.')\n",
    "            plt.scatter(regcoord[:,1], regcoord[:,0], c=intensity, cmap='jet')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.show()\n",
    "        \n",
    "#         domregion['region' + str(i)]['pc_id']=pc\n",
    "        domregion['region' + str(i)]['threshold.area']=props.area\n",
    "        domregion['region' + str(i)]['threshold.perc']=props.area/np.sum(image)\n",
    "        domregion['region' + str(i)]['mass.total']=totalmass\n",
    "        domregion['region' + str(i)]['mass.region']=np.nansum(intensity)\n",
    "        domregion['region' + str(i)]['mass.perc']=np.nansum(intensity)/totalmass\n",
    "        domregion['region' + str(i)]['region.centroid.0']=props.centroid[0]\n",
    "        domregion['region' + str(i)]['region.centroid.1']=props.centroid[1]\n",
    "        domregion['region' + str(i)]['region.orient']=props.orientation\n",
    "        domregion['region' + str(i)]['region.majaxis']=props.major_axis_length\n",
    "        domregion['region' + str(i)]['region.minaxis']=props.minor_axis_length\n",
    "        domregion['region' + str(i)]['region.extent']=props.extent\n",
    "        domregion['region' + str(i)]['region.eccentricity']=props.eccentricity\n",
    "        if props.minor_axis_length > 0:\n",
    "            domregion['region' + str(i)]['region.majmin.ratio']=props.major_axis_length/props.minor_axis_length\n",
    "\n",
    "        if verbose:\n",
    "            print('Threshold area: ', props.area)\n",
    "            print('Percent threshold area assessed: ', props.area/np.sum(image))\n",
    "            print('\\nTotalmass: ', totalmass)\n",
    "            print('Areamass: ', np.nansum(intensity))\n",
    "            print('Percent mass: ', np.nansum(intensity)/totalmass)\n",
    "            print('\\nCentroid: ', props.centroid)\n",
    "            print('Orientation :', props.orientation)\n",
    "            print('Major axis: ', props.major_axis_length)\n",
    "            print('Minor axis: ', props.minor_axis_length)\n",
    "            print('Extent: ',props.extent)\n",
    "            print('Eccentricty: ',props.eccentricity)\n",
    "\n",
    "    return domregion\n",
    "\n",
    "    \n",
    "pc_id =1\n",
    "\n",
    "eigenbrain = rebuildEigenbrain(eig_vec, pc_id, roimask=roimask, eigb_shape=None, \n",
    "    maskind=None)\n",
    "\n",
    "eigenbrain = eigenbrain * flipped[pc_id]\n",
    "\n",
    "comx, comy = centerOfMass(eigenbrain, \n",
    "                          threshold=thresh[pc_id], \n",
    "                          plot=True)\n",
    "# xm, ym = xyProjectMax(eigenbrain)\n",
    "# xa, ya = positionMaxIntensity(eigenbrain)\n",
    "\n",
    "k = spatialCharacterize(eigenbrain, pc=pc_id, threshold=thresh[pc_id])\n",
    "print(k)\n",
    "l = sortNestedDict(k, sortkey = 'mass.perc')\n",
    "print(l)\n",
    "\n",
    "\n",
    "# xmax, ymax, major, minor = eigenbrainCOM(eigenbrain, roimask, thresh[pc_id])\n",
    "\n",
    "# plt.imshow(eigenbrain)\n",
    "# plt.colorbar()\n",
    "# plt.scatter(ymax, xmax, color='w', marker='*' )\n",
    "# # plt.plot([x_v2*-scale+ np.mean(x), x_v2*scale+ np.mean(x)],\n",
    "# #              [y_v2*-scale+ np.mean(y), y_v2*scale+ np.mean(y)]\n",
    "# plt.plot(major[1], major[0], color = 'black')\n",
    "# plt.plot(minor[1], minor[0], color = 'red')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for i in range(eig_vec.shape[1]):\n",
    "    if i == 0:\n",
    "        data = pd.DataFrame()\n",
    "    if i%25 == 0:\n",
    "        print('Working on {0} of {1} PCs'.format(i, eig_vec.shape[1]))\n",
    "    pc_id = i\n",
    "    \n",
    "    #Spatial\n",
    "    #rebuild the eigenbrain\n",
    "    eigenbrain = rebuildEigenbrain(eig_vec, pc_id, roimask=roimask, eigb_shape=None, \n",
    "        maskind=None)\n",
    "    eigenbrain = eigenbrain * flipped[i]\n",
    "    \n",
    "    #general characteristics\n",
    "    std = np.nanstd(eigenbrain)\n",
    "    avg = np.nanmean(eigenbrain)\n",
    "    mina = np.nanmin(eigenbrain)\n",
    "    maxa = np.nanmax(eigenbrain)\n",
    "    comxall, comyall = centerOfMass(eigenbrain, \n",
    "                          plot=False, verbose = False)\n",
    "    comxdom, comydom = centerOfMass(eigenbrain, \n",
    "                              threshold=thresh[i], \n",
    "                              plot=False, verbose = False)\n",
    "    \n",
    "    #characterize the largest domain in the threshold\n",
    "    k = spatialCharacterize(eigenbrain, pc=pc_id, threshold=thresh[i], plot = False)\n",
    "#     print(k)\n",
    "    l = sortNestedDict(k, sortkey = 'mass.perc')\n",
    "#     print(l)\n",
    "    \n",
    "    if len(l) > 0:\n",
    "        for key, val in k[l[0]].items():\n",
    "            data.at[i, key] = val\n",
    "    data.at[i,'spatial.std'] = std\n",
    "    data.at[i,'spatial.avg'] = avg        \n",
    "    data.at[i,'spatial.min'] = mina        \n",
    "    data.at[i,'spatial.max'] = maxa        \n",
    "    data.at[i,'spatial.COMall.x'] = comxall      \n",
    "    data.at[i,'spatial.COMall.y'] = comyall        \n",
    "    data.at[i,'spatial.COMdom.x'] = comxdom      \n",
    "    data.at[i,'spatial.COMdom.y'] = comydom\n",
    "    \n",
    "    #Temporal\n",
    "    #run wavelet analysis on it\n",
    "    w = wave(data = tcourse[i],\n",
    "             fps = fps,\n",
    "             mother = 'MORLET',\n",
    "             param = 4,\n",
    "             siglvl = 0.95,\n",
    "             verbose = False,\n",
    "             plot=False)\n",
    "    w.globalWaveletSpectrum()\n",
    "    \n",
    "    ratio, index = findSig()\n",
    "    m = findContinBool(index)\n",
    "    m = temporalCharacterize(m, ratio, w.flambda)\n",
    "    n = sortNestedDict(m, sortkey = 'freq.rangesz')\n",
    "    if len(n) > 0:\n",
    "        for key, val in m[n[0]].items():\n",
    "            if key == 'freq.index':\n",
    "                pass\n",
    "            else:\n",
    "                data.at[i, key] = val\n",
    "    data.at[i,'temporal.autocorr'] = tca.lagNAutoCorr(tcourse[i], 1)\n",
    "    data.at[i,'temporal.min'] = np.nanmin(tcourse[i])\n",
    "    data.at[i, 'temporal.max'] = np.nanmax(tcourse[i])\n",
    "    data.at[i,'temporal.std'] = np.nanstd(tcourse[i])\n",
    "\n",
    "data.sort_index()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save file for future manipulations\n",
    "tsv_output_file = '/home/brian/Documents/data/Classifier/' + file[:9] + '_ica_metrics.tsv'\n",
    "data.to_csv(tsv_output_file, sep = '\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics that are currently being used are:\n",
    "\n",
    "### Spatial measures of ICs:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ** basic measures **\n",
    "1. **spatial standard deviation ['spatial.std']**\n",
    "2. spatial average data.at[i,'spatial.avg'] _not needed_\n",
    "3. spatial global minima ['spatial.min']        \n",
    "4. **spatial global maxima ['spatial.max']**\n",
    "\n",
    "    ** center of mass/centroid ** _all similar measures, choose only one that works well_\n",
    "5. spatial center of mass (x value) of whole IC ['spatial.COMall.x']      \n",
    "6. spatial center of mass (y value) of whole IC ['spatial.COMall.y']      \n",
    "7. spatial center of mass (x value) of only thresholded IC domain ['spatial.COMdom.x']      \n",
    "8. spatial center of mass (y value) of only thresholded IC domain ['spatial.COMdom.y']\n",
    "9. **_the first dimension of the centroid of the largest masked domain, non-weighted value ['region.centroid.0']_**\n",
    "10. **_the second dimension of the centroid of the largest masked domain, non-weighted value ['region.centroid.1']_**\n",
    "\n",
    "    ** size of domain **\n",
    "11. number of pixels in the largest of the thresholded IC domains ['threshold.area'] in pixels\n",
    "\n",
    "    ** weight of domain/IC component **\n",
    "12. sum of the absolute value of all pixels within the IC ['mass.total']\n",
    "13. **sum of the intensity vales in the thresholded IC domian ['mass.region']**\n",
    "14. precent of intensity values in the thresholed IC domain with respect to the absolute value of the sum of all values ['mass.perc'] _depends on ['mass.total]_\n",
    "\n",
    "    ** domain properties **\n",
    "15. the angle of the major axis through the largest masked domain ['region.orient']\n",
    "16. the length of the major axis through the largest masked domain ]['region.majaxis']\n",
    "17. **_the length of the minor axis through the largest masked domain ]['region.minaxis']_**\n",
    "\n",
    "    ** shape of domain ** _all are dependent on the minor and major axis_\n",
    "18. the proportion of the pixels in the bounding box of the region that are also in the region ['region.extent']\n",
    "19. the eccentricity of the ellipse that has the same second-moments as the region ['region.eccentricity']\n",
    "20. the ratio of the major/minor axis of the region ['region.majmin.ratio']\n",
    "\n",
    "### Temporal measures of ICs:\n",
    "signal = frequency power based on wavlet analysis after running globalWaveletSpectrum\n",
    "\n",
    "noise = null hypothesis of red noise from globalWaveletSpectrum\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ** basic measures **\n",
    "1. first autocorrelation ['temporal.autocorr'] _used in noise component selection_\n",
    "2. timeseries minimum ['temporal.min']\n",
    "3. **timeseries maximum ['temporal.max']**\n",
    "4. **timeseries standard deviation ['temporal.std']**\n",
    "\n",
    "    ** global Wavelet Spectrum SNR/integration **\n",
    "5. the largest signal to noise ratio within the longest frequency segment ['freq.maxsnr']\n",
    "6. the average signal to noise ratio within the longest frequency segment ['freq.avgsnr']\n",
    "7. the lowest frequency (highest period) of the longest frequency segment ['freq.range.low']\n",
    "8. **the higest frequency (lowest period) of the longest frequency segment ['freq.range.high']**\n",
    "9. the distance beteween the highest and lowest frequencies ['freq.rangesz']\n",
    "10. **the approximate integration of the signal above the noise threshold (default using right sided estimation) ['freq.integrate']**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Values selected for classifier:</b>\n",
    "<p>**Spatial** <p>\n",
    "1. spatial.std  \n",
    "2. spatial.max     \n",
    "3. mass.region      \n",
    "4. region.minaxis    \n",
    "\n",
    "<p> **Temporal** <p>\n",
    "1. temporal.std   \n",
    "2. temporal.max      \n",
    "3. freq.range.high   \n",
    "4. freq.integrate  \n",
    "5. freq.rangesz\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> Shreya
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "/Users/jcrndm/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \n"
=======
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  import sys\n"
>>>>>>> Shreya
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/brian/Documents/data/Classifier/170804_02_ica_metrics.tsv' does not exist: b'/home/brian/Documents/data/Classifier/170804_02_ica_metrics.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[0;32m<ipython-input-4-ad4575fdcf7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtsv_output_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ica_metrics.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsv_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exp_ic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[0;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[1;32m   1899\u001b[0m                         \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1901\u001b[0;31m                         infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[1;32m   1902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
=======
      "\u001b[0;32m<ipython-input-3-4ffa7ac7940b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtsv_output_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ica_metrics.tsv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsv_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exp_ic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[0;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[1;32m   1899\u001b[0m                         \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1901\u001b[0;31m                         infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[1;32m   1902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1903\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
>>>>>>> Shreya
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/brian/Documents/data/Classifier/170804_02_ica_metrics.tsv' does not exist: b'/home/brian/Documents/data/Classifier/170804_02_ica_metrics.tsv'"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Emma
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Open file\n",
    "path = '/home/brian/Documents/data/Classifier/'\n",
    "file = '170804_02_ica.hdf5'\n",
    "\n",
    "tsv_output_file = path + file[:9] + '_ica_metrics.tsv'\n",
    "data = pd.DataFrame.from_csv(tsv_output_file, sep = '\\t', index_col='exp_ic')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 1,
>>>>>>> Shreya
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[0;32m<ipython-input-3-cbe69d0f58c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcol_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#puts the paramter on a scale of 0 to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
=======
      "\u001b[0;32m<ipython-input-1-cbe69d0f58c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcol_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#puts the paramter on a scale of 0 to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
>>>>>>> Shreya
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Emma
   "source": [
    "%matplotlib inline\n",
    "\n",
    "col_list = list(data)\n",
    "\n",
    "#puts the paramter on a scale of 0 to 1\n",
    "\n",
    "datacopy = data.drop('age', axis =1).copy()\n",
    "datacopy -= datacopy.min()\n",
    "datacopy /= datacopy.max()\n",
    "datacopy['age'] = data['age']\n",
    "data = datacopy.fillna(value=0)\n",
    "del datacopy\n",
    "\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 7,
   "metadata": {},
>>>>>>> Shreya
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-48f0d19ad090>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnotnoise_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'noise'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnotnoise_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# def sepHist(data, artifact, noise):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
>>>>>>> Emma
   "source": [
    "\n",
    "notnoise_index = data.loc[data['noise'] < 1].index\n",
    "data = data.loc[notnoise_index]\n",
    "\n",
    "# def sepHist(data, artifact, noise):\n",
    "    \n",
    "#     noisedf = data[classify == 2]\n",
    "#     artifactdf = data[classify == 1]\n",
    "#     sigdf = data[classify == 0]\n",
    "#     return noisedf, artifactdf, sigdf\n",
    "\n",
    "# noisedf, artifactdf, sigdf = sepHist(data, artifact, noise)\n",
    "\n",
    "artifact_index = data.loc[data['artifact'] == 1].index\n",
    "signal_index = data.loc[data['signal'] == 1].index\n",
    "\n",
    "for col in col_list:\n",
    "    print(col)\n",
    "\n",
    "    fig , ax1 = plt.subplots()\n",
    "    ax1.hist(data.loc[artifact_index, col], bins = 25, alpha = 0.5, label ='artifact', color = 'g')\n",
    "    ax1.grid(False)         \n",
    "    ax1.set_xlabel('Values')\n",
    "    ax1.set_ylabel('# of instances', color = 'green')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.hist(data.loc[signal_index, col], bins = 25, alpha = 0.5, label ='signal', color = 'b')\n",
    "    ax2.set_ylabel('# of instances', color = 'blue')\n",
    "    ax2.grid(False)\n",
    "#     ax2.hist(noisedf[col].dropna(), bins = 25, alpha = 0.5, label ='noise', color = 'r')\n",
    "#     ax2.set_xlabel('Values')\n",
    "#     ax2.set_ylabel('# of instances', color = 'red')\n",
    "    \n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "#     fig = plt.figure()\n",
    "#     x = data.loc[(data[col]) & (noise[:X_reduced_tsne.shape[0]] == 0)]\n",
    "#     y = data.iloc[[noise[:X_reduced_tsne.shape[0]] == 1],col]\n",
    "#     ax = fig.add_subplot(111)\n",
    "#     _ = ax.hist(x)\n",
    "#     _ = ax.hist(y, color ='red', alpha=0.3)\n",
    "    \n",
    "# #     data[col,np.where(noise[:X_reduced_tsne.shape[0]] == 0)]\n",
    "# #     d\n",
    "#     plt.show()\n",
    "\n",
    "# for col in col_list:\n",
    "#     print(col)\n",
    "#     data3[col].apply(np.log2).hist(by = noise[:X_reduced_tsne.shape[0]])\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data2.at[np.where(classify == 2)[0], 'class'] = 'noise'\n",
    "# data2.loc[np.where(classify == 1)[0], 'class'] = 'artifact'\n",
    "# data2.loc[np.where(classify == 0)[0], 'class'] = 'signal'\n",
    "\n",
    "data.groupby('signal').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('signal').std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling using SMOTE\n",
    "\n",
    "from this article: https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "\n",
    "With our training data created, I’ll up-sample the no-subscription using the SMOTE algorithm(Synthetic Minority Oversampling Technique). At a high level, SMOTE:\n",
    "\n",
    "1. Works by creating synthetic samples from the minor class (no-subscription) instead of creating copies.\n",
    "2. Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observations.\n",
    "\n",
    "We are going to implement SMOTE in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "y = data.loc[:,'signal'].values\n",
    "\n",
    "# y = np.squeeze(pd.DataFrame(np.ndarray.astype(y, int), columns=['signal']))\n",
    "X = data.loc[:, data.columns != 'signal'].fillna(value=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n",
    "\n",
    "#augment your data\n",
    "os = SMOTE(random_state=42)\n",
    "\n",
    "columns = X_train.columns\n",
    "os_data_X, os_data_y=os.fit_sample(X_train, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X, columns=columns)\n",
    "os_data_y = pd.DataFrame(data = os_data_y, columns =['signal'])\n",
    "\n",
    "print('Length of data augmentation:', len(os_data_X))\n",
    "print('Number of addition singal points: {0} \\t\\tProportion: {1}'.format(len(os_data_y[os_data_y['signal'] == 1]), len(os_data_y[os_data_y['signal'] == 1])/len(os_data_X)))\n",
    "print('Number of addition non-singal points: {0} \\tProportion: {1}'.format(len(os_data_y[os_data_y['signal'] == 0]), len(os_data_y[os_data_y['signal'] == 0])/len(os_data_X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "Recursive Feature Elimination (RFE) is based on the idea to repeatedly construct a model and choose either the best or worst performing feature, setting the feature aside and then repeating the process with the rest of the features. This process is applied until all features in the dataset are exhausted. The goal of RFE is to select features by recursively considering smaller and smaller sets of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "data_final_vars = X.columns.values.tolist()\n",
    "y_vars = ['signal']\n",
    "\n",
    "rfe = RFE(logreg, 20)\n",
    "rfe = rfe.fit(os_data_X, os_data_y.values.ravel())\n",
    "\n",
    "print('Support\\tRanking\\tVariable')\n",
    "for i, var in enumerate(data_final_vars):\n",
    "    print(rfe.support_[i], '\\t',rfe.ranking_[i],'\\t', var)\n",
    "\n",
    "new_vars = X.columns.values[rfe.ranking_ == 1].tolist()\n",
    "# new_vars.append('class')\n",
    "print(new_vars)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0c21ad046714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# data2.at[np.where(classify == 2)[0], 'class'] = 'noise'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Both temporal and spatial metrics visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
>>>>>>> 68e81ca8baf1eb1a271d73383f95377bc5b0985d
   "source": [
    "\n",
    "\n",
    "# metrics = ['spatial.std', 'spatial.max', 'mass.region', 'region.minaxis', \n",
    "#            'temporal.std', 'temporal.max', 'freq.range.high', 'freq.integrate', #'freq.rangesz', \n",
    "#            'class']\n",
    "\n",
    "# data2.at[np.where(classify == 2)[0], 'class'] = 'noise'\n",
    "\n",
    "data3 = data[new_vars]\n",
    "\n",
    "# Both temporal and spatial metrics visualize  \n",
    "sns.pairplot(data3, \n",
    "             diag_kind=\"kde\", \n",
    "             markers=[\"o\", \"s\"], \n",
    "             hue_order = [0, 1], hue='artifact')\n",
    "\n",
    "plt.savefig('/home/brian/Documents/figures/p21_pairplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# del new_vars[-1]\n",
    "                #spatial metrics\n",
    "new_new_vars = ['region.orient', 'mass.region', 'region.minaxis',\n",
    "                'threshold.area', 'region.majaxis',\n",
    "                'spatial.std', 'spatial.min', 'spatial.max', \n",
    "                #temporal metrics\n",
    "                'freq.maxsnr.freq', 'temporal.autocorr', 'temporal.min', 'temporal.max']\n",
    "\n",
    "\n",
    "logit_model=sm.Logit(y,X[new_vars])\n",
    "results=logit_model.fit()\n",
    "print(results.summary2())\n",
    "\n",
    "# take out the non-significant variables?  What about running this on all the values that we measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial metrics visualize\n",
    "sns.pairplot(data2.fillna(value=0)[['spatial.std', 'spatial.max', 'mass.region', 'region.minaxis']])\n",
    "\n",
    "# plt.savefig('/home/brian/Documents/figures/min_highfreq_pairplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal metrics visualize  \n",
    "sns.pairplot(data2.fillna(value=0)[['temporal.std', 'temporal.max', 'freq.range.high', 'freq.integrate', 'freq.rangesz' ]])\n",
    "\n",
    "# plt.savefig('/home/brian/Documents/figures/min_highfreq_pairplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_new_vars = [#spetial metrics \n",
    "                'spatial.std', 'spatial.min', 'spatial.max', \n",
    "                 'mass.perc', 'region.minaxis', 'region.majaxis', 'region.orient', 'threshold.area',\n",
    "                #temporal metrics\n",
    "                'temporal.max', 'freq.maxsnr.freq', 'length', 'freq.rangesz']\n",
    "\n",
    "classConfidence = pd.DataFrame(index=X.index)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[new_new_vars], y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "gnb = GaussianNB(var_smoothing= 0.25)\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "print('Accuracy of Gaussian NB classifier on test set: {:.2f}'.format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "col_list = ['not pred.', 'pred.']\n",
    "row_list = ['not act.', 'act.']\n",
    "row_format =\"{:>15}\" * (len(col_list) + 1)\n",
    "print(row_format.format(\"\", *col_list))\n",
    "for rlist, row in zip(row_list, confusion_matrix):\n",
    "    print(row_format.format(rlist, *row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, gnb.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, gnb.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Gaussian NB (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "\n",
    "classConfidence.loc[X_test.index, 'gnb_prob'] = gnb.predict_proba(X_test)[:,1]\n",
    "classConfidence.loc[X_train.index, 'gnb_prob'] = gnb.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logtistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "logreg = LogisticRegression(C=1, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "col_list = ['not pred.', 'pred.']\n",
    "row_list = ['not act.', 'act.']\n",
    "row_format =\"{:>15}\" * (len(col_list) + 1)\n",
    "print(row_format.format(\"\", *col_list))\n",
    "for rlist, row in zip(row_list, confusion_matrix):\n",
    "    print(row_format.format(rlist, *row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n",
    "\n",
    "classConfidence.loc[X_test.index, 'logreg_prob'] = logreg.predict_proba(X_test)[:,1]\n",
    "classConfidence.loc[X_train.index, 'logreg_prob'] = logreg.predict_proba(X_train)[:,1]\n",
    "\n",
    "# print(data.loc[X_test.index, 'signal'])\n",
    "\n",
    "# print(logreg.predict_proba(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classifer with Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(kernel=\"rbf\", gamma=0.01, C=50, random_state=42, probability= True)\n",
    "svm_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'.format(svm_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "col_list = ['not pred.', 'pred.']\n",
    "row_list = ['not act.', 'act.']\n",
    "row_format =\"{:>15}\" * (len(col_list) + 1)\n",
    "print(row_format.format(\"\", *col_list))\n",
    "for rlist, row in zip(row_list, confusion_matrix):\n",
    "    print(row_format.format(rlist, *row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, svm_clf.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svm_clf.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='SVC (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('SVM_ROC.svg')\n",
    "plt.show()\n",
    "\n",
    "classConfidence.loc[X_test.index, 'SVC_prob'] = svm_clf.predict_proba(X_test)[:,1]\n",
    "classConfidence.loc[X_train.index, 'SVC_prob'] = svm_clf.predict_proba(X_train)[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_features = len(new_new_vars), random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy of Random Forest classifier on test set: {:.2f}'.format(rnd_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "col_list = ['not pred.', 'pred.']\n",
    "row_list = ['not act.', 'act.']\n",
    "row_format =\"{:>15}\" * (len(col_list) + 1)\n",
    "print(row_format.format(\"\", *col_list))\n",
    "for rlist, row in zip(row_list, confusion_matrix):\n",
    "    print(row_format.format(rlist, *row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, rnd_clf.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rnd_clf.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Random Trees Classifier (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('rnd_ROC.svg')\n",
    "plt.show()\n",
    "\n",
    "classConfidence.loc[X_test.index, 'rnd_clf_prob'] = rnd_clf.predict_proba(X_test)[:,1]\n",
    "classConfidence.loc[X_train.index, 'rnd_clf_prob'] = rnd_clf.predict_proba(X_train)[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression(C=1, solver = 'lbfgs', random_state = 42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 50, max_features = len(new_new_vars), random_state=42)\n",
    "svm_clf = SVC(kernel=\"rbf\", gamma=0.01, C=50, random_state=42, probability= True)\n",
    "gnb = GaussianNB(var_smoothing= 0.25)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf), ('gnb', gnb_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "print('Accuracy of Voting classifier on test set: {:.2f}'.format(voting_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "col_list = ['not pred.', 'pred.']\n",
    "row_list = ['not act.', 'act.']\n",
    "row_format =\"{:>15}\" * (len(col_list) + 1)\n",
    "print(row_format.format(\"\", *col_list))\n",
    "for rlist, row in zip(row_list, confusion_matrix):\n",
    "    print(row_format.format(rlist, *row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, voting_clf.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, voting_clf.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Voting Classifier (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('rnd_ROC.svg')\n",
    "plt.show()\n",
    "\n",
    "classConfidence.loc[X_test.index, 'voting_clf_prob'] = voting_clf.predict_proba(X_test)[:,1]\n",
    "classConfidence.loc[X_train.index, 'voting_clf_prob'] = voting_clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "\n",
    "# print(voting_clf.predict_proba(X_test))\n",
    "# print(voting_clf.predict_proba(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "logreg = LogisticRegression(C=1, solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "\n",
    "svm_clf = SVC(kernel=\"rbf\", gamma=0.01, C=50, random_state=42, probability= True)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, svm_clf.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, svm_clf.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr, tpr, label='SVC (area = %0.2f)' % roc_auc)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_features = len(new_new_vars), random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, rnd_clf.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, rnd_clf.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr, tpr, label='Random Forest Classifier (area = %0.2f)' % roc_auc)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, voting_clf.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, voting_clf.predict_proba(X_test)[:,1])\n",
    "plt.plot(fpr, tpr, label='Voting Classifier (area = %0.2f)' % roc_auc)\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([-0.025, 1.0])\n",
    "plt.ylim([.50, 1.025])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.savefig('rnd_ROC.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classConfidence.plot(y=['logreg_prob', 'SVC_prob', 'rnd_clf_prob', 'gnb_prob', 'voting_clf_prob'],\n",
    "                     figsize=(20,4), rot = 45, grid=True)\n",
    "plt.ylabel('artifact                                       signal')\n",
    "plt.savefig(path + 'classifier_prob.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Dimension Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# all data without exclusion\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from operator import add\n",
    "import plotly\n",
    "\n",
    "plt.rc(\"font\", size=14)\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "# a = np.arange(data.shape[0]).astype(str).tolist()\n",
    "# a = ['domain ' + str(i) for i in a]\n",
    "# b = ['  class ' + str(i) for i in classify]\n",
    "\n",
    "# text = list(map(add, a, b))\n",
    "\n",
    "tsne = TSNE(n_components=2, \n",
    "            perplexity=20, # between 5 and 50 only \n",
    "            learning_rate = 300,\n",
    "            random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(X[new_new_vars])\n",
    "\n",
    "\n",
    "color = pd.DataFrame(index=data.index)\n",
    "\n",
    "y_test_pred = voting_clf.predict(X_test)\n",
    "y_train_pred = voting_clf.predict(X_train)\n",
    "\n",
    "X_train_index = X_train.index.values\n",
    "X_test_index = X_test.index.values\n",
    "\n",
    "y_train_value = np.squeeze(y_train)\n",
    "y_test_value = np.squeeze(y_test)\n",
    "\n",
    "color['class'] = np.zeros(len(color))\n",
    "print(color.head())\n",
    "\n",
    "\n",
    "for j, ind in enumerate(X_train_index):\n",
    "    if y_train_pred[j]==0:\n",
    "        color.at[ind, 'class'] += 4\n",
    "    if y_train_pred[j] != y_train_value[j]: # make odd if inaccurate prediction\n",
    "        color.at[ind, 'class'] += 1\n",
    "\n",
    "for j, ind in enumerate(X_test_index):\n",
    "    color.at[ind, 'class'] +=2\n",
    "    if y_test_pred[j]==0:\n",
    "        color.at[ind, 'class'] += 4\n",
    "    if y_test_pred[j] != y_test_value[j]:\n",
    "        color.at[ind, 'class'] += 1\n",
    "        \n",
    "        \n",
    "print('\\nIndices used to train that are signal and predicted as signal')\n",
    "print(*color.loc[color['class']==0].index, sep='\\t')\n",
    "\n",
    "print('\\nIndices used to train that are signal and are predicted as NOT signal')\n",
    "print(*color.loc[color['class']==1].index, sep='\\t')\n",
    "\n",
    "print('\\nIndices used to test that are signal and and are predicted as signal')\n",
    "print(*color.loc[color['class']==2].index, sep='\\t')\n",
    "\n",
    "print('\\nIndices used to test that are signal and are predicted as NOT signal')\n",
    "print(*color.loc[color['class']==3].index, sep='\\t')\n",
    "\n",
    "print('\\nIndices used to train that are NOT signal and are predicted as NOT signal')\n",
    "print(*color.loc[color['class']==4].index, sep='\\t')\n",
    "\n",
    "print('\\nIndices used to train that are NOT signal and are predicted as signal')\n",
    "print(*color.loc[color['class']==5].index, sep='\\t')\n",
    "\n",
    "print('\\nIndices used to test that are NOT signal and are predicted as NOT signal')\n",
    "print(*color.loc[color['class']==6].index, sep='\\t')\n",
    "\n",
    "print('\\nIndices used to test that are NOT signal and are predicted as signal')\n",
    "print(*color.loc[color['class']==7].index, sep='\\t')\n",
    "\n",
    "\n",
    "## NOT WORKING!!\n",
    "# l = []\n",
    "# trace0= go.Scatter(\n",
    "#     x= X_reduced_tsne[:, 0],\n",
    "#     y= X_reduced_tsne[:, 1],\n",
    "#     mode= 'markers',\n",
    "#     marker= dict(size= 20,\n",
    "#                 line= dict(width=1),\n",
    "#                 colorbar =dict(title='effectiveness of training',\n",
    "#                                 tickvals = [1,3,5,7],\n",
    "#                                 ticktext = ['Signal train','Not signal train','Signal test', 'Not signal test'],\n",
    "# #                                 opacity= 0.7,\n",
    "# #                                 cmax =12\n",
    "#                ),\n",
    "#     color= color,\n",
    "#     colorscale=[[0, 'rgb(170,205,225)'],[1/8, 'rgb(32,120,185)'], [2/8, 'rgb(175,226,132)'],[3/8, 'rgb(50,162,42)'],\n",
    "#                 [4/8, 'rgb(255,151,157)'], [5/8, 'rgb(222,28,29)'], [6/8, 'rgb(253,189,109)'], [7/8, 'rgb(255,125,2)']],\n",
    "# #      name= 'Domain',\n",
    "#                 ))#,hoverinfo = text)\n",
    "# # The hover text goes here...\n",
    "\n",
    "\n",
    "# layout= go.Layout(\n",
    "#     title= 't-SNE',\n",
    "#     hovermode= 'closest',\n",
    "#     xaxis= dict(\n",
    "#         title= 't-sne1',\n",
    "#         ticklen= 5,\n",
    "#         zeroline= False,\n",
    "# #         gridwidth= 2,\n",
    "#         showgrid = True\n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title= 't-sne2',\n",
    "#         ticklen= 5,\n",
    "# #         gridwidth= 2,\n",
    "#         zeroline = False,\n",
    "#         showgrid =True\n",
    "\n",
    "#     ),\n",
    "#     showlegend= False,\n",
    "# )\n",
    "\n",
    "# fig = go.Figure(data=[trace0], layout=layout)\n",
    "# py.iplot(fig, filename='jupyter-Freq vs minor axis of Brain regions')\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize = (20,20))\n",
    "plt.title('t-SNE', fontsize=14)\n",
    "cax = plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=color['class'], cmap='Paired', vmax = 12)\n",
    "plt.xlabel(\"t-SNE_1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE_2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "cbar = fig.colorbar(cax, ticks=np.arange(0,12,2))\n",
    "cbar.ax.set_xticklabels(['train signal', 'train other', 'test signal', 'test other'])  # horizontal colorbar\n",
    "# plt.colorbar()\n",
    "# plt.savefig(\"other_dim_reduction_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the classifier\n",
    "path = '/home/brian/Documents/data/Classifier/'\n",
    "file = '170804_02_ica.hdf5'\n",
    "\n",
    "print(path + 'p21_classifier.sav')\n",
    "\n",
    "voting_clf = joblib.load(path + 'p21_classifier.sav')\n",
    "\n",
    "new_new_vars = ['region.orient', 'mass.region', 'region.minaxis',\n",
    "                'threshold.area', 'region.majaxis',\n",
    "                'spatial.std', 'spatial.min', 'spatial.max', \n",
    "                #temporal metrics\n",
    "                'freq.maxsnr.freq', 'temporal.autocorr', 'temporal.min', 'temporal.max']\n",
    "\n",
    "h = h5(path + 'p21_classifier.hdf5')\n",
    "h.save({'classifier':voting_clf, 'metrics':new_new_vars})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within the same animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifer\n",
    "loaded_model = joblib.load(path + 'p21_classifier.sav')\n",
    "\n",
    "# test a new dataset\n",
    "#Open file\n",
    "path = '/home/brian/Documents/data/Classifier/'\n",
    "file = '170804_03_ica.hdf5'\n",
    "\n",
    "tsv_output_file = path + file[:9] + '_ica_metrics.tsv'\n",
    "new_data = pd.DataFrame.from_csv(tsv_output_file, sep = '\\t')\n",
    "\n",
    "# Load the noise and artifact components\n",
    "g = h5(path + file)\n",
    "yselected = g.load('artifact_components')\n",
    "noise = g.load('noise_components')\n",
    "\n",
    "signal = np.where(noise!=1)[0]\n",
    "\n",
    "#puts the paramter on a scale of 0 to 1\n",
    "new_data -= new_data.min()\n",
    "new_data /= new_data.max()\n",
    "\n",
    "X_new = new_data.fillna(value=0).loc[signal, new_new_vars]\n",
    "predict_y = loaded_model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updating dataframe\n",
    "#Open file\n",
    "path = '/home/brian/Documents/data/Classifier/'\n",
    "file = '170804_02_ica.hdf5'\n",
    "\n",
    "tsv_output_file = path + file[:9] + '_ica_metrics.tsv'\n",
    "data = pd.DataFrame.from_csv(tsv_output_file, sep = '\\t')\n",
    "\n",
    "#Load classifications\n",
    "h = h5(path + file)\n",
    "print(h.keys())\n",
    "noise = h.load('noise_components')\n",
    "artifact = h.load('artifact_components')\n",
    "\n",
    "noise = noise*2\n",
    "classify = artifact + noise\n",
    "classify[classify==3] = 1\n",
    "\n",
    "nois_index = np.where(classify == 2)\n",
    "art_index = np.where(classify ==1)\n",
    "sig_index = np.where(classify == 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yselected = yselected[signal]\n",
    "\n",
    "print('Accuracy of Voting classifier on new dataset: {:.2f}'.format(loaded_model.score(X_new, np.abs(yselected.astype('float32')-1))))\n",
    "p_y = np.abs(predict_y-1)\n",
    "\n",
    "plt.plot(np.abs(yselected.astype('float32')-1))\n",
    "plt.plot(yselected-p_y)\n",
    "plt.show()\n",
    "\n",
    "dif = yselected-p_y\n",
    "print(np.where(dif!=0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Between animals of the same age and genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifer\n",
    "loaded_model = joblib.load(path + 'p21_classifier.sav')\n",
    "\n",
    "# test a new dataset\n",
    "#Open file\n",
    "path = '/home/brian/Documents/data/Classifier/'\n",
    "file = '171003_05_ica.hdf5' #other P21 data\n",
    "\n",
    "tsv_output_file = path + file[:9] + '_ica_metrics.tsv'\n",
    "new_data = pd.DataFrame.from_csv(tsv_output_file, sep = '\\t')\n",
    "\n",
    "# Load the noise and artifact components\n",
    "g = h5(path + file)\n",
    "yselected = g.load('artifact_components')\n",
    "noise = g.load('noise_components')\n",
    "\n",
    "signal = np.where(noise!=1)[0]\n",
    "\n",
    "#puts the paramter on a scale of 0 to 1\n",
    "new_data -= new_data.min()\n",
    "new_data /= new_data.max()\n",
    "\n",
    "X_new = new_data.fillna(value=0).loc[signal, new_new_vars]\n",
    "predict_y = loaded_model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yselected = yselected[signal]\n",
    "\n",
    "print('Accuracy of Voting classifier on new dataset: {:.2f}'.format(loaded_model.score(X_new, np.abs(yselected.astype('float32')-1))))\n",
    "p_y = np.abs(predict_y-1)\n",
    "\n",
    "plt.plot(np.abs(yselected.astype('float32')-1))\n",
    "plt.plot(yselected-p_y)\n",
    "plt.show()\n",
    "\n",
    "dif = yselected-p_y\n",
    "print('Signal that was identified as artifact')\n",
    "print(np.where(dif<0)[0])\n",
    "print('\\nArtifact that was identified as Signal')\n",
    "print(np.where(dif>0)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between animals of a different age and genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classifer\n",
    "loaded_model = joblib.load(path + 'p21_classifier.sav')\n",
    "\n",
    "# test a new dataset\n",
    "#Open file\n",
    "path = '/home/brian/Documents/data/Classifier/'\n",
    "file = '171003_01_ica.hdf5' # ephrin a5ko p6\n",
    "\n",
    "tsv_output_file = path + file[:9] + '_ica_metrics.tsv'\n",
    "new_data = pd.DataFrame.from_csv(tsv_output_file, sep = '\\t')\n",
    "\n",
    "# Load the noise and artifact components\n",
    "g = h5(path + file)\n",
    "yselected = g.load('artifact_components')\n",
    "noise = g.load('noise_components')\n",
    "\n",
    "signal = np.where(noise!=1)[0]\n",
    "\n",
    "#puts the paramter on a scale of 0 to 1\n",
    "new_data -= new_data.min()\n",
    "new_data /= new_data.max()\n",
    "\n",
    "X_new = new_data.fillna(value=0).loc[signal, new_new_vars]\n",
    "predict_y = loaded_model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data without exclusion\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "data2 = data.fillna(value=0).copy()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(data2.values)\n",
    "\n",
    "plt.title('t-SNE', fontsize=14)\n",
    "plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=classify[:X_reduced_tsne.shape[0]], cmap='jet')\n",
    "plt.xlabel(\"t-SNE_1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE_2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "\n",
    "# save_fig(\"other_dim_reduction_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include = ['mass.perc','region.orient', 'threshold.perc', 'region.eccentricity', \n",
    "           'temporal.autocorr','temporal.std','freq.integrate', 'freq.avgsnr','freq.range.low',\n",
    "          'freq.range.high']\n",
    "\n",
    "data3 = data[include].copy()\n",
    "\n",
    "data3['relative.position.x'] = data['spatial.COMdom.x']/roimask.shape[0]\n",
    "data3['relative.position.y'] = data['spatial.COMdom.y']/roimask.shape[0]\n",
    "data3['ratio.axis'] = data['region.minaxis']/data['region.majaxis']\n",
    "\n",
    "#spatial.COMdom.x/roimask.shape[0]\n",
    "#spatial.COMdom.y/roimask.shape[1]\n",
    "#region.minaxis/region.majaxis\n",
    "for item in data3:\n",
    "    print(item)\n",
    "\n",
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.fillna(value=0, inplace = True)\n",
    "data3 -= data3.min()\n",
    "data3 /= data3.max()\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(data3.values)\n",
    "\n",
    "plt.title('t-SNE', fontsize=14)\n",
    "plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=np.arange(X_reduced_tsne.shape[0]), cmap='jet')\n",
    "plt.xlabel(\"t-SNE_1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE_2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.title('t-SNE', fontsize=14)\n",
    "plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=classify[:X_reduced_tsne.shape[0]], cmap='jet')\n",
    "plt.xlabel(\"t-SNE_1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE_2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "# save_fig(\"other_dim_reduction_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include = ['freq.maxsnr', \n",
    "           'freq.avgsnr',\n",
    "           'length',\n",
    "           'spatial.max',\n",
    "           'spatial.std', \n",
    "           'region.minaxis', \n",
    "           'region.extent', \n",
    "           'region.eccentricity', \n",
    "           'temporal.autocorr',\n",
    "           'freq.integrate',\n",
    "           'mass.region', \n",
    "           'mass.perc',\n",
    "           'temporal.std',\n",
    "           'threshold.perc',\n",
    "           'temporal.max',\n",
    "           'temporal.min',\n",
    "           'freq.range.high',\n",
    "           'freq.range.low',\n",
    "           'region.majmin.ratio']\n",
    "\n",
    "data4 = data[include].copy()\n",
    "\n",
    "#spatial.COMdom.x/roimask.shape[0]\n",
    "#spatial.COMdom.y/roimask.shape[1]\n",
    "#region.minaxis/region.majaxis\n",
    "# print(list(data))\n",
    "\n",
    "# data4.apply(np.log2).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.fillna(value=0, inplace = True)\n",
    "\n",
    "data4 -= data4.min()\n",
    "data4 /= data4.max()\n",
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "\n",
    "data5 = data4.loc[(classify == 1)|(classify == 0)]\n",
    "c = classify[(classify == 1)|(classify == 0)]\n",
    "\n",
    "X_reduced_tsne = tsne.fit_transform(data5.values)\n",
    "\n",
    "plt.title('t-SNE', fontsize=14)\n",
    "plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=np.arange(X_reduced_tsne.shape[0]), cmap='jet')\n",
    "plt.xlabel(\"t-SNE_1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE_2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.title('t-SNE', fontsize=14)\n",
    "plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=c, cmap='jet')\n",
    "plt.xlabel(\"t-SNE_1\", fontsize=18)\n",
    "plt.ylabel(\"t-SNE_2\", fontsize=18)\n",
    "plt.grid(True)\n",
    "# save_fig(\"other_dim_reduction_plot\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# X_reduced_tsne = tsne.fit_transform(data4.values)\n",
    "\n",
    "# plt.title('t-SNE', fontsize=14)\n",
    "# plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=np.arange(X_reduced_tsne.shape[0]), cmap='jet')\n",
    "# plt.xlabel(\"t-SNE_1\", fontsize=18)\n",
    "# plt.ylabel(\"t-SNE_2\", fontsize=18)\n",
    "# plt.grid(True)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "# plt.title('t-SNE', fontsize=14)\n",
    "# plt.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=classify, cmap='jet')\n",
    "# plt.xlabel(\"t-SNE_1\", fontsize=18)\n",
    "# plt.ylabel(\"t-SNE_2\", fontsize=18)\n",
    "# plt.grid(True)\n",
    "# # save_fig(\"other_dim_reduction_plot\")\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Three groups\n",
    "group1 = np.where(X_reduced_tsne[:, 0] < -20)\n",
    "print('\\nNoise and blood artifacts: \\n', group1)\n",
    "# group1b = np.where((X_reduced_tsne[:, 0] < 0) & (X_reduced_tsne[:, 0] > -9))\n",
    "# print('\\n\\tClosest blood components to neuronal: \\n\\t', group1b)\n",
    "group2 = np.where((X_reduced_tsne[:, 0] < -20) & (c == 1))\n",
    "print('\\t\\nNoise components labeled artifact: \\n\\t', group2)\n",
    "group3 = np.where((X_reduced_tsne[:, 0] < -20) & (c == 0))\n",
    "print('\\t\\nNoise components labeled as signal: \\n\\t', group3)\n",
    "\n",
    "group4 = np.where(X_reduced_tsne[:, 0] > -20)\n",
    "print('\\nNeural components: \\n', group4)\n",
    "group5 = np.where((X_reduced_tsne[:, 0] > -20) & (c == 1))\n",
    "print('\\n\\tNeural components labeled artifact: \\n\\t', group5)\n",
    "group6 = np.where((X_reduced_tsne[:, 0] > -20) & (c == 0))\n",
    "print('\\n\\tNueral components labeled as signal: \\n\\t', group6)\n",
    "\n",
    "group7 = np.where((X_reduced_tsne[:, 0] > -20) & (X_reduced_tsne[:, 1] > -10))\n",
    "print('\\n\\tNueral components labeled as signal: \\n\\t', group7)\n",
    "group8 = np.where((X_reduced_tsne[:, 0] > -20) & (X_reduced_tsne[:, 1] > -10)  & (c == 1))\n",
    "print('\\n\\tNueral components labeled as artifact: \\n\\t', group8)\n",
    "group9 = np.where((X_reduced_tsne[:, 0] > -20) & (X_reduced_tsne[:, 1] > -10)  & (c == 0))\n",
    "print('\\n\\tNueral components labeled as signal: \\n\\t', group9)\n",
    "\n",
    "\n",
    "group7 = np.where((X_reduced_tsne[:, 0] >- 20) & (X_reduced_tsne[:, 1] < -10))\n",
    "print('\\n\\tNueral components labeled as signal: \\n\\t', group7)\n",
    "group8 = np.where((X_reduced_tsne[:, 0] >- 20) & (X_reduced_tsne[:, 1] < -10)  & (c == 1))\n",
    "print('\\n\\tNueral components labeled as artifact: \\n\\t', group8)\n",
    "group9 = np.where((X_reduced_tsne[:, 0] >- 20) & (X_reduced_tsne[:, 1] < -10)  & (c == 0))\n",
    "print('\\n\\tNueral components labeled as signal: \\n\\t', group9)\n",
    "\n",
    "# group3a = np.where((X_reduced_tsne[:, 0] > -5) & (X_reduced_tsne[:, 1] < -3))\n",
    "# group3b = np.where((X_reduced_tsne[:, 0] > -5) & (X_reduced_tsne[:, 0] < 5) & (X_reduced_tsne[:, 1] > -3))\n",
    "# group3c = np.where((X_reduced_tsne[:, 0] > 10))\n",
    "# print('\\n\\tLowest group of neural components (visual cortex): \\n\\t', group3a)\n",
    "# print('\\n\\tMiddle group of neural components (OB and autitory): \\n\\t', group3b)\n",
    "# print('\\n\\tHighest group of neural compoents (the rest) \\n\\t', group3c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
