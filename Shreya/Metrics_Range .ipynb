{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from matplotlib import pyplot as plt\n",
    "from hdf5manager import hdf5manager as h5\n",
    "from scipy.stats import mode\n",
    "#from derivativeEventDetection import detectSpikes, FixedQueue\n",
    "#from eventCoincidence import eventCoin, _displayInfo, visualizeProgress\n",
    "import ctypes as c\n",
    "from multiprocessing import Process, Array, cpu_count, Manager\n",
    "import pandas as pd\n",
    "import time\n",
    "from waveletAnalysis import waveletAnalysis as wave\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchBinarize(  ROI_timecourses, #\n",
    "                    fps = 10,\n",
    "                    window_size = 50, \n",
    "                    plot = False\n",
    "                    ):\n",
    "    \n",
    "    print('Binarizing ROI_timecourses.')\n",
    "    \n",
    "    binarizedData = np.zeros_like(ROI_timecourses, dtype='uint8')\n",
    "    eventMat = np.zeros_like(ROI_timecourses, dtype='uint8')\n",
    "\n",
    "    for i , datarow in enumerate(ROI_timecourses):\n",
    "        # start_time = time.clock()\n",
    "        w = wave(datarow, fps=fps, siglvl = 0.99)\n",
    "        spikes, vals = w.waveletEventDetection()\n",
    "        e = []\n",
    "        spike_del = []\n",
    "        for j, v in enumerate(vals):\n",
    "            found = False\n",
    "            possible = np.where(np.array(0.25*v < ROI_timecourses[i,spikes[j]-window_size :spikes[j]+window_size ]))[0] + spikes[j] - window_size \n",
    "            for k, g in groupby(enumerate(possible), lambda x: x[0] - x[1]):\n",
    "                l = list(map(itemgetter(1), g))\n",
    "                if spikes[j] in l:\n",
    "                    found = True\n",
    "                    e.extend(l)\n",
    "            if not found:\n",
    "                spike_del.append(j)\n",
    "        spikes = np.delete(spikes, spike_del)\n",
    "        binarizedData[i,spikes] = 1\n",
    "        eventMat[i,np.unique(e)] = 1\n",
    "\n",
    "    if plot:\n",
    "        t = np.arange(ROI_timecourses.shape[1])/fps\n",
    "\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        gs = GridSpec(5, 4) # 2 rows, 3 columns\n",
    "        ax1=fig.add_subplot(gs[0,:]) \n",
    "        ax2=fig.add_subplot(gs[1,:], sharex=ax1)\n",
    "        ax3=fig.add_subplot(gs[2,:], sharex=ax1)\n",
    "        ax4=fig.add_subplot(gs[3,:], sharex=ax1)\n",
    "        ax5=fig.add_subplot(gs[4,:], sharex=ax1)\n",
    "\n",
    "        ax1.plot(t, ROI_timecourses[0] * 100, label='dFoF', c = 'b')\n",
    "        ax1.plot(t, eventMat[0]+1, label = 'eventMat', c = 'r')\n",
    "        ax1.eventplot(np.where(binarizedData[0] > 0.5)[0]* 1/fps, label = 'waveEvent', lineoffsets = -1, linelengths=0.5, color='k')\n",
    "        ax1.set_ylabel('dfof {0}'.format(0))\n",
    "        plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "        # ax2.plot(t, ROI_timecourses[1], label='dFoF', c = 'b')\n",
    "        # ax2.plot(t, eventMat[1]+1, label = 'eventMat', c = 'r')\n",
    "        # ax2.eventplot(np.where(binarizedData[1] > 0.5)[0] * 1/fps, label = 'waveEvent', lineoffsets = -1, linelengths=0.5, color='k')\n",
    "        # ax2.set_ylabel('dfof {0}'.format(1))\n",
    "        # plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "\n",
    "        # ax3.plot(t, ROI_timecourses[2], label='dFoF', c = 'b')\n",
    "        # ax3.plot(t, eventMat[2]+1, label = 'eventMat', c = 'r')\n",
    "        # ax3.eventplot(np.where(binarizedData[2] > 0.5)[0] * 1/fps, label = 'waveEvent', lineoffsets = -1, linelengths=0.5, color='k')\n",
    "        # ax3.set_ylabel('dfof {0}'.format(2))\n",
    "        # plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "\n",
    "        # ax4.plot(t, ROI_timecourses[3], label='dFoF', c = 'b')\n",
    "        # ax4.plot(t, eventMat[3]+1, label = 'eventMat', c = 'r')\n",
    "        # ax4.eventplot(np.where(binarizedData[3] > 0.5)[0] * 1/fps, label = 'waveEvent', lineoffsets = -1, linelengths=0.5, color='k')\n",
    "        # ax4.set_ylabel('dfof {0}'.format(3))\n",
    "        # plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "\n",
    "        # ax5.plot(t, ROI_timecourses[4], label='dFoF', c = 'b')\n",
    "        # ax5.plot(t, eventMat[4]+1, label = 'eventMat', c = 'r')\n",
    "        # ax5.eventplot(np.where(binarizedData[4] > 0.5)[0] * 1/fps, label = 'waveEvent', lineoffsets = -1, linelengths=0.5, color='k')\n",
    "        # ax5.set_ylabel('dfof {0}'.format(4))\n",
    "        # ax5.set_xlabel('time (s)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return binarizedData, eventMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Characterizes each timecourse in a matrix of brain data by number of events, event frequency, maximum event magnitude\n",
    "#and average inter-event interval (time between consecutive events). Each event in each timecourse is further characterized\n",
    "#by its magnitude and duration\n",
    "def eventCharacterization(ROI_timecourses, binarizedData, eventMat):\n",
    "    # max_events = 0\n",
    "    # min_events = 1000\n",
    "\n",
    "    print('Characterizing events.')\n",
    "\n",
    "    def findMiddle(input_list):\n",
    "        middle = float(len(input_list))/2\n",
    "        if middle % 2 != 0:\n",
    "            return input_list[int(middle - .5)]\n",
    "        else:\n",
    "            return input_list[int(middle)]\n",
    "\n",
    "    combineEvent = eventMat + binarizedData\n",
    "\n",
    "    m_dict = dict(event_dur = [0],\n",
    "                interevent_length = [0])\n",
    "    master = [dict() for x in range(ROI_timecourses.shape[0])]\n",
    "    for i, eventRow in enumerate(combineEvent):\n",
    "        eventFrames = np.where(eventRow > 0.5)[0]\n",
    "        j=0\n",
    "        for k, g in groupby(enumerate(eventFrames), lambda x: x[0] - x[1]):\n",
    "            l = list(map(itemgetter(1), g))\n",
    "            if j == 0:\n",
    "                master[i]['start_event_loc'] = [l[0]]\n",
    "                master[i]['mid_event_loc'] = [findMiddle(l)]\n",
    "                master[i]['inter_event_fr_dur'] = [l[0]]\n",
    "                master[i]['event_fr_dur'] = [len(l)]\n",
    "                master[i]['n_wave_events'] = [np.sum([eventRow[l] > 1])]\n",
    "                master[i]['event_max_amp'] = [np.max(ROI_timecourses[i,l])]\n",
    "                master[i]['event_min_amp'] = [np.min(ROT_timecourses[i,1])]\n",
    "                master[i]['event_avg_amp'] = [np.mean(ROI_timecourses[i,l])]\n",
    "                ''' \n",
    "                should we cluster these after characterizing to determine burst and single events.  These parameters will change\n",
    "                with respect to age.\n",
    "                '''\n",
    "                # if master[i]['n_wave_events'][-1] < 3: \n",
    "                if master[i]['n_wave_events'][-1] < 3 or (master[i]['event_fr_dur'][-1] < 20 and master[i]['n_wave_events'][-1] == 2):\n",
    "                    master[i]['event_type'] = ['s'] # single\n",
    "                else:\n",
    "                    master[i]['event_type'] = ['b'] # burst\n",
    "                prev_last_ind = l[-1]\n",
    "            else:\n",
    "                master[i]['start_event_loc'].append(l[0])\n",
    "                master[i]['mid_event_loc'].append(findMiddle(l))\n",
    "                master[i]['inter_event_fr_dur'] = [l[0]-1-prev_last_ind]\n",
    "                master[i]['event_fr_dur'].append(len(l))\n",
    "                master[i]['n_wave_events'].append(np.sum([eventRow[l] > 1]))\n",
    "                master[i]['event_max_amp'].append(np.max(ROI_timecourses[i,l]))\n",
    "                master[i]['event_min_amp'].append(np.min(ROT_timecourses[i,1]))\n",
    "                master[i]['event_avg_amp'].append(np.mean(ROI_timecourses[i,l]))\n",
    "                # if master[i]['n_wave_events'][-1] < 3:\n",
    "                if master[i]['n_wave_events'][-1] < 3 or (master[i]['event_fr_dur'][-1] < 20 and master[i]['n_wave_events'][-1] == 2):\n",
    "                    master[i]['event_type'].append('s') # single\n",
    "                else:\n",
    "                    master[i]['event_type'].append('b') # burst\n",
    "                prev_last_ind = l[-1]\n",
    "            j+=1 \n",
    "\n",
    "        master[i]['n_events'] = j\n",
    "    # for row in master:               \n",
    "    #     print(row)\n",
    "    # print(master[0].keys())\n",
    "    return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8f005c14a1d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/shreyamantripragada/Documents/brain_data.hdf5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdfof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfof_mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h5' is not defined"
     ]
    }
   ],
   "source": [
    "path2 = '/Users/shreyamantripragada/Documents/brain_data.hdf5'\n",
    "k = h5(path2)\n",
    "k.keys()\n",
    "dfof = k.load('dfof_mean')\n",
    "\n",
    "\n",
    "binarizedData, eventMat = batchBinarize(dfof)\n",
    "# plt.plot(eventMat[0])\n",
    "\n",
    "win_size = 100\n",
    "\n",
    "win_mean = np.convolve(dfof, np.ones(win_size)/win_size, mode = 'same')\n",
    "\n",
    "flat_dfof = dfof - win_mean\n",
    "\n",
    "threshold = np.zeros(dfof.shape)\n",
    "threshold[dfof>np.std(dfof)] = 1\n",
    "\n",
    "plt.plot(dfof)\n",
    "plt.plot(win_mean)\n",
    "plt.plot(flat_dfof)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "'''\n",
    "Functions for classifying ICA domains\n",
    "\n",
    "Authors: Sydney C. Weiser and Brian R. Mullen\n",
    "Date: 2019-04-06\n",
    "'''\n",
    "\n",
    "import wholeBrain as wb\n",
    "import numpy as np\n",
    "import scipy\n",
    "# import math\n",
    "# import seaborn as sns\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "from skimage.measure import label, regionprops\n",
    "from waveletAnalysis import waveletAnalysis as wave\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "try:\n",
    "    from hdf5manager import hdf5manager as h5\n",
    "except Exception as e:\n",
    "    print('Error importing hdf5manager.py')\n",
    "    print('\\t ERROR : ', e)\n",
    "\n",
    "try:\n",
    "    import timecourseAnalysis as tca\n",
    "except Exception as e:\n",
    "    print('Error importing timecourseAnalysis.py')\n",
    "    print('\\t ERROR : ', e)\n",
    "\n",
    "\n",
    "def sortNoise(timecourses, return_logpdf=False, method='KDE'):\n",
    "    '''\n",
    "    Sorts timecourses into two clusters (signal and noise) based on \n",
    "    lag-1 autocorrelation.  \n",
    "    Timecourses should be a np array of shape (n, t).\n",
    "\n",
    "    Returns noise_components, a np array with 1 value for all noise \n",
    "    timecourses detected, as well as the cutoff value detected\n",
    "    '''\n",
    "    noise_components = np.zeros((timecourses.shape[0],), dtype='uint8')\n",
    "\n",
    "    if method == 'KDE':\n",
    "        from sklearn.neighbors import KernelDensity\n",
    "        from scipy.signal import argrelextrema\n",
    "\n",
    "        # calculate lag autocorrelations\n",
    "        lag1 = tca.lagNAutoCorr(timecourses, 1)\n",
    "\n",
    "        # calculate minimum between min and max peaks\n",
    "        kde_skl = KernelDensity(\n",
    "            kernel='gaussian', bandwidth=0.05).fit(lag1[:, np.newaxis])\n",
    "        x_grid = np.linspace(-0.2,1.2,1200)\n",
    "\n",
    "        log_pdf = kde_skl.score_samples(x_grid[:, np.newaxis])\n",
    "\n",
    "        maxima = argrelextrema(np.exp(log_pdf), np.greater)[0]\n",
    "        if len(maxima) <= 1:\n",
    "            print('Only one cluster found')\n",
    "            cutoff = 0\n",
    "        else:\n",
    "            cutoff_index = np.argmin(np.exp(log_pdf)[maxima[0]:maxima[-1]]) \\\n",
    "                + maxima[0]\n",
    "            cutoff = x_grid[cutoff_index]\n",
    "            print('autocorr cutoff:', cutoff)\n",
    "\n",
    "        noise_components[:] = (lag1 < cutoff).astype('uint8')\n",
    "    else:\n",
    "        raise Exception('method: {0} is unknown!'.format(method))\n",
    "\n",
    "    if return_logpdf:\n",
    "        return noise_components, cutoff, log_pdf\n",
    "    else:\n",
    "        return noise_components, cutoff\n",
    "\n",
    "\n",
    "def findContinBool(boolArray1D):\n",
    "    nestDict = {}\n",
    "    j, k = (-1, 0)\n",
    "    usek = True\n",
    "    for i in range(len(boolArray1D)):\n",
    "        if boolArray1D[i] and not boolArray1D[i-1]:\n",
    "            j += 1 \n",
    "            if j >= 1:\n",
    "                nestDict['region' + str(j-1)]['length'] = k\n",
    "            k = 0\n",
    "            nestDict['region' + str(j)] = {}\n",
    "            nestDict['region' + str(j)]['freq.index'] = [] \n",
    "            nestDict['region' + str(j)]['freq.index'].append(i)\n",
    "            k += 1\n",
    "        if boolArray1D[i] and boolArray1D[i-1]:\n",
    "            if i == 1:\n",
    "                j += 1\n",
    "                nestDict['region' + str(j)] = {}\n",
    "                nestDict['region' + str(j)]['freq.index'] = []\n",
    "                nestDict['region' + str(j)]['freq.index'].append(i-1)\n",
    "                nestDict['region' + str(j)]['freq.index'].append(i)\n",
    "            elif i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                k += 1\n",
    "                nestDict['region' + str(j)]['freq.index'].append(i)\n",
    "\n",
    "    if j != -1:\n",
    "        nestDict['region' + str(j)]['length'] = k\n",
    "    \n",
    "    return nestDict\n",
    "\n",
    "\n",
    "def findSig():\n",
    "    ratio = np.squeeze(w.gws/w.gws_sig)\n",
    "    index = (ratio > 1)\n",
    "    return ratio, index\n",
    "\n",
    "\n",
    "def approxIntegrate(index, freq, power, sigcutoff): \n",
    "    #power\n",
    "    diff = np.squeeze(power - sigcutoff) #integrate only significant area\n",
    "    #freq\n",
    "    if index[-1] != freq.shape[0]-1: # default to right sided estimation\n",
    "        offset = [x+1 for x in index]\n",
    "        binsz = freq[index] - freq[offset]\n",
    "    else:\n",
    "        offset = [x-1 for x in index]\n",
    "        binsz = freq[offset] - freq[index]\n",
    "        \n",
    "    return np.sum(binsz * diff[index])\n",
    "\n",
    "\n",
    "def temporalCharacterize(sigfreq, ratio, freq):\n",
    "    for k in sigfreq.keys():\n",
    "        sigfreq[k]['freq.maxsnr'] = np.nanmax(ratio[sigfreq[k]['freq.index']])\n",
    "        sigfreq[k]['freq.maxsnr.freq'] = np.nanargmax(ratio[sigfreq[k]['freq.index']])\n",
    "        sigfreq[k]['freq.avgsnr'] = np.nanmean(ratio[sigfreq[k]['freq.index']])\n",
    "        sigfreq[k]['freq.range.low'] = freq[sigfreq[k]['freq.index'][-1]] \n",
    "        sigfreq[k]['freq.range.high'] = freq[sigfreq[k]['freq.index'][0]]\n",
    "        if sigfreq[k]['freq.range.low'] and not sigfreq[k]['freq.range.high']:\n",
    "                sigfreq[k]['freq.range.high'] = 5\n",
    "        if sigfreq[k]['freq.range.high'] and not sigfreq[k]['freq.range.low']:\n",
    "                sigfreq[k]['freq.range.low'] = 0                \n",
    "        sigfreq[k]['freq.rangesz'] = (sigfreq[k]['freq.range.high'] - sigfreq[k]['freq.range.low'])\n",
    "        sigfreq[k]['freq.integrate'] = approxIntegrate(index = sigfreq[k]['freq.index'], freq = w.flambda, \n",
    "                                                  power = w.gws, sigcutoff = w.gws_sig)\n",
    "\n",
    "    return sigfreq\n",
    "\n",
    "\n",
    "def sortNestedDict(nestDict, sortkey = 'freq.rangesz'):\n",
    "    sortDict = []\n",
    "    for k in nestDict.keys(): sortDict.append(k)\n",
    "#     sortDict = sorted(sortDict, key=lambda x: (sigfreq[x]['length']), reverse =True)\n",
    "    sortDict = sorted(sortDict, key=lambda x: (nestDict[x][sortkey]), reverse =True)\n",
    "\n",
    "    return sortDict\n",
    "\n",
    "\n",
    "def centerOfMass(eigenbrain, threshold=None, verbose=False, plot=False):\n",
    "    eigen = eigenbrain.copy()\n",
    "    if threshold is not None:\n",
    "        eigen[eigen < threshold] = np.nan\n",
    "    x, y = np.where(np.isnan(eigen)==False)\n",
    "\n",
    "    #total sum\n",
    "    totalmass = np.nansum(np.abs(eigen))\n",
    "    #weighted sum\n",
    "    sumrmx = 0\n",
    "    sumrmy = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        sumrmx += np.abs(eigenbrain[x[i],y[i]])*x[i]\n",
    "        sumrmy += np.abs(eigenbrain[x[i],y[i]])*y[i]\n",
    "\n",
    "#     plt.imshow(eigen)\n",
    "#     plt.show()\n",
    "    \n",
    "    comx = sumrmx/totalmass\n",
    "    comy = sumrmy/totalmass\n",
    "   \n",
    "    if verbose:\n",
    "        print('xcom: ', comx)\n",
    "        print('ycom: ', comy)\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(eigenbrain)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(comy, comx, color='w', marker='*' )\n",
    "        plt.show()\n",
    "\n",
    "    return comx, comy\n",
    "\n",
    "\n",
    "def xyProjectMax(eigenbrain, verbose=True, plot=True):\n",
    "    xmean = np.nanmean(eigenbrain, axis = 1)\n",
    "    xmax = np.nanargmax(xmean)\n",
    "    \n",
    "    ymean = np.nanmean(eigenbrain, axis = 0)\n",
    "    ymax = np.nanargmax(ymean)\n",
    "\n",
    "    if verbose:\n",
    "        print('xmax: ', xmax)\n",
    "        print('ymax: ', ymax)\n",
    "        \n",
    "    if plot:\n",
    "        plt.plot(ymean)\n",
    "        plt.plot(xmean)\n",
    "        plt.scatter([xmax,ymax],[0,0], color = 'r')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        plt.imshow(eigenbrain)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(ymax, xmax, color='w', marker='*' )\n",
    "        plt.show()\n",
    "\n",
    "    return xmax, ymax\n",
    "\n",
    "    \n",
    "def positionMaxIntensity(eigenbrain, verbose=True, plot=True):\n",
    "    amax = np.nanmax(eigenbrain)\n",
    "    xamax, yamax = np.where(eigenbrain == amax)\n",
    "    if verbose:\n",
    "        print('x amax: ', xamax)\n",
    "        print('y amax: ', yamax)\n",
    "        \n",
    "    if plot:\n",
    "        plt.imshow(eigenbrain)\n",
    "        plt.colorbar()\n",
    "        plt.scatter(yamax, xamax, color='w', marker='*' )\n",
    "        plt.show()\n",
    "    return xamax, yamax\n",
    "    \n",
    "\n",
    "def spatialCharacterize(eigenbrain, pc, threshold, verbose = False, plot = False):\n",
    "    eigen = eigenbrain.copy()\n",
    "#     eigen[eigen == np.nan] = 0\n",
    "    eigen[eigen < threshold] = np.nan  \n",
    "    x, y = np.where(np.isnan(eigen)==False)\n",
    "    image = np.zeros_like(eigenbrain)\n",
    "    image[x,y] = 1\n",
    "    image = scipy.ndimage.median_filter(image, size=5)\n",
    "    label_img = label(image)\n",
    "    regions = regionprops(label_img)\n",
    "    totalmass = np.nansum(np.abs(eigenbrain))\n",
    "    domregion = {}\n",
    "    \n",
    "    for i, props in enumerate(regions):\n",
    "        domregion['region' + str(i)] = {}\n",
    "        regcoord = props.coords\n",
    "        intensity = np.zeros_like(regcoord[:,0]).astype('float16')\n",
    "        for j, coord in enumerate(regcoord):\n",
    "            intensity[j] = eigenbrain[coord[0], coord[1]]\n",
    "        \n",
    "        if plot:\n",
    "            plt.scatter(y, x, color = (0,0,0,0.05), marker='.')\n",
    "            plt.scatter(regcoord[:,1], regcoord[:,0], c=intensity, cmap='jet')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.show()\n",
    "        \n",
    "#         domregion['region' + str(i)]['pc_id']=pc\n",
    "        domregion['region' + str(i)]['threshold.area']=props.area\n",
    "        domregion['region' + str(i)]['threshold.perc']=props.area/np.sum(image)\n",
    "        domregion['region' + str(i)]['mass.total']=totalmass\n",
    "        domregion['region' + str(i)]['mass.region']=np.nansum(intensity)\n",
    "        domregion['region' + str(i)]['mass.perc']=np.nansum(intensity)/totalmass\n",
    "        domregion['region' + str(i)]['region.centroid.0']=props.centroid[0]\n",
    "        domregion['region' + str(i)]['region.centroid.1']=props.centroid[1]\n",
    "        domregion['region' + str(i)]['region.orient']=props.orientation\n",
    "        domregion['region' + str(i)]['region.majaxis']=props.major_axis_length\n",
    "        domregion['region' + str(i)]['region.minaxis']=props.minor_axis_length\n",
    "        domregion['region' + str(i)]['region.extent']=props.extent\n",
    "        domregion['region' + str(i)]['region.eccentricity']=props.eccentricity\n",
    "        if props.minor_axis_length > 0:\n",
    "            domregion['region' + str(i)]['region.majmin.ratio']=props.major_axis_length/props.minor_axis_length\n",
    "\n",
    "        if verbose:\n",
    "            print('Threshold area: ', props.area)\n",
    "            print('Percent threshold area assessed: ', props.area/np.sum(image))\n",
    "            print('\\nTotalmass: ', totalmass)\n",
    "            print('Areamass: ', np.nansum(intensity))\n",
    "            print('Percent mass: ', np.nansum(intensity)/totalmass)\n",
    "            print('\\nCentroid: ', props.centroid)\n",
    "            print('Orientation :', props.orientation)\n",
    "            print('Major axis: ', props.major_axis_length)\n",
    "            print('Minor axis: ', props.minor_axis_length)\n",
    "            print('Extent: ',props.extent)\n",
    "            print('Eccentricty: ',props.eccentricity)\n",
    "\n",
    "    return domregion\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import argparse\n",
    "    import time\n",
    "    import pandas \n",
    "    from wholeBrainPCA import rebuildEigenbrain\n",
    "\n",
    "    # Argument Parsing\n",
    "    # -----------------------------------------------\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('-i', '--input', type = argparse.FileType('r'), \n",
    "        nargs = '+', required = False, \n",
    "        help = 'path to the processed ica file(s)')\n",
    "    ap.add_argument('-f', '--fps', default = 10, required = False,\n",
    "        help = 'frames per second from recordings')\n",
    "    ap.add_argument('-n', '--noise_only', action='store_true',\n",
    "        help = 'deterime noise components and save in hdf5')\n",
    "    ap.add_argument('-t','--train', action='store_true',\n",
    "        help = 'train the classifier on the newest class_metric dataframe')\n",
    "    ap.add_argument('-ud', '--updateDF', action='store_true',\n",
    "        help = 'update full classifier dataframe')\n",
    "    ap.add_argument('-uc', '--updateClass', action='store_true',\n",
    "        help = 'update class in experimental dataframe, input ica.hdf5 and ensure metrics.tsv is in the same folder')\n",
    "    ap.add_argument('-fc', '--force', action='store_true',\n",
    "        help = 'force re-calculation')\n",
    "    ap.add_argument('-p', '--plot', action='store_true',\n",
    "        help= 'Vizualize training outcome')\n",
    "    args = vars(ap.parse_args())\n",
    "\n",
    "    classMetricsPath = '/home/brian/Documents/data/Classifier/class_metrics.tsv'\n",
    "    # classifier = '/home/brian/Documents/data/Classifier/p21_classifier.sav'\n",
    "    classifier = '/home/brian/Documents/data/Classifier/p21_classifier.hdf5'\n",
    "\n",
    "    if args['input'] != None:\n",
    "        paths = [path.name for path in args['input']]\n",
    "        print('Input found:')\n",
    "        [print('\\t'+path) for path in paths]\n",
    "\n",
    "        for path in paths:\n",
    "            print('Processing file:', path)\n",
    "\n",
    "            if path.endswith('.hdf5'):\n",
    "                assert path.endswith('_ica.hdf5') | path.endswith('_ica_reduced.hdf5'),\\\n",
    "                     \"Path did not end in '_ica.hdf5'\"\n",
    "                savepath = path.replace('.hdf5', '_metrics.tsv')\n",
    "                savepath = savepath.replace('_reduced', '')\n",
    "                base = os.path.basename(path)\n",
    "\n",
    "                print('\\nLoading data to create classifier metrics\\n------------------------------------------------')\n",
    "                f = h5(path)\n",
    "                f.print()\n",
    "\n",
    "                if ('noise_components' not in f.keys()) | args['force']:\n",
    "                    print('Calculating Noise Components')\n",
    "                    noise, cutoff = sortNoise(f.load('timecourses'))\n",
    "                    f.save({'noise_components':noise, 'cutoff':cutoff})\n",
    "                else:\n",
    "                    #Load data from ica.hdf5 file \n",
    "                    noise = f.load('noise_components')\n",
    "\n",
    "                if args['noise_only']:\n",
    "                    continue\n",
    "\n",
    "                if 'artifact_components' in f.keys():\n",
    "                    artifact = f.load('artifact_components')\n",
    "                    if np.sum(artifact) != 0:\n",
    "                        comb = noise + artifact\n",
    "                        artifact[comb == 2] = 0 #if it is id'd as noise and artifact, keep as noise \n",
    "                        signal = np.array(comb == 0).astype(int)\n",
    "                    else:\n",
    "                        signal = np.zeros_like(noise)\n",
    "                else:\n",
    "                    artifact = np.zeros_like(noise)\n",
    "                    signal = np.zeros_like(noise)\n",
    "\n",
    "                notnoise_index = (noise==0)\n",
    "\n",
    "                if args['updateClass'] and np.sum(signal)!=0 and os.path.exists(savepath):\n",
    "                    data = pd.DataFrame.from_csv(savepath, sep = '\\t', index_col='exp_ic')\n",
    "                    print (data)\n",
    "                    print('\\nImporting dataframe\\n------------------------------------')\n",
    "                    print('Sum of each component BEFORE update:\\n',  data[['artifact','signal']].sum())\n",
    "                    data['artifact'] = artifact[notnoise_index]\n",
    "                    data['signal'] = signal[notnoise_index]\n",
    "                    print('\\nSum of each component AFTER update:\\n', data[['artifact','signal']].sum())\n",
    "                                    #Save file for future manipulations\n",
    "                    print('Saving dataframe to:', savepath)\n",
    "                    data.to_csv(savepath, sep = '\\t')\n",
    "\n",
    "                if (os.path.exists(savepath) and args['force']) or (not os.path.exists(savepath)):\n",
    "                    if args['force']:\n",
    "                        print('Re-calculating the metrics.')\n",
    "                    if args['updateClass']:\n",
    "                        print('Unable to update experimental dataframe. Either no artifact components or no metrics.tsv found')\n",
    "                        print('Continuing on to create dataframe')\n",
    "                    flipped = f.load('flipped')\n",
    "                    tcourse = f.load('timecourses')\n",
    "                    roimask = f.load('roimask')\n",
    "                    eig_vec = f.load('eig_vec')\n",
    "                    thresh = f.load('thresholds')\n",
    "                    try:\n",
    "                         meta = f.load('expmeta')\n",
    "                    except Exception as e:\n",
    "                        print('Unable to add age to the dataFrame')\n",
    "                        print('\\t ERROR : ', e)                   \n",
    "\n",
    "                    #flipped the inverted timeseries and \n",
    "                    tcourse = (np.multiply(tcourse.T, flipped)).T\n",
    "                    eig_vec = np.multiply(eig_vec, flipped)\n",
    "\n",
    "                    #create the dataframe and set up indices\n",
    "                    data = pd.DataFrame()\n",
    "\n",
    "\n",
    "                    data['artifact'] = artifact[notnoise_index]\n",
    "                    data['signal'] = signal[notnoise_index]\n",
    "                    data['age'] = np.ones(int(signal[notnoise_index].shape[0])) * int(re.findall(r'\\d+',meta['meta']['anml']['age'])[0])\n",
    "                    data['exp_ic'] = [base[:-9] + '-' + '{}'.format(str(i).zfill(4)) for i in np.where(noise == 0)[0].tolist()]\n",
    "                    data = data.set_index('exp_ic')\n",
    "\n",
    "                    print('\\nCalculating spatial metrics\\n------------------------------------------------')\n",
    "\n",
    "                    data['spatial.std'] = np.nanstd(eig_vec[:, notnoise_index], axis = 0)\n",
    "                    data['spatial.avg'] = np.nanmean(eig_vec[:, notnoise_index], axis = 0)\n",
    "                    data['spatial.min'] = np.nanmin(eig_vec[:, notnoise_index], axis = 0)\n",
    "                    data['spatial.max'] = np.nanmax(eig_vec[:, notnoise_index], axis = 0)\n",
    "\n",
    "                    for i, pid in enumerate(data.index.tolist()):\n",
    "                        if i%50 == 0:\n",
    "                            print('Working on {0} of {1} components'.format(i, len (data)))\n",
    "                        pc_id = i\n",
    "                        \n",
    "                        #rebuild the eigenbrain\n",
    "                        eigenbrain = rebuildEigenbrain(eig_vec, pc_id, roimask=roimask, eigb_shape=None, \n",
    "                            maskind=None)\n",
    "                        \n",
    "                        comxall, comyall = centerOfMass(eigenbrain)\n",
    "                        comxdom, comydom = centerOfMass(eigenbrain, threshold=thresh[i])\n",
    "                        \n",
    "                        #characterize the largest domain in the threshold\n",
    "                        k = spatialCharacterize(eigenbrain, pc=pc_id, threshold=thresh[i], plot = False)\n",
    "\n",
    "                        l = sortNestedDict(k, sortkey = 'mass.perc')\n",
    "                        data.at[pid,'spatial.n.domains'] = len(l)\n",
    "                        if len(l) > 0:\n",
    "                            for key, val in k[l[0]].items():\n",
    "                                data.at[pid, key] = val\n",
    "              \n",
    "                        data.at[pid,'spatial.COMall.x'] = comxall      \n",
    "                        data.at[pid,'spatial.COMall.y'] = comyall        \n",
    "                        data.at[pid,'spatial.COMdom.x'] = comxdom      \n",
    "                        data.at[pid,'spatial.COMdom.y'] = comydom\n",
    "\n",
    "                    print('\\nCalculating temporal metrics\\n------------------------------------------------')\n",
    "\n",
    "                    data['temporal.autocorr'] = tca.lagNAutoCorr(tcourse[notnoise_index,:], 1)\n",
    "                    data['temporal.min'] = np.nanmin(tcourse[notnoise_index,:], axis = 1)\n",
    "                    data['temporal.max'] = np.nanmax(tcourse[notnoise_index,:], axis = 1)\n",
    "                    data['temporal.std'] = np.nanstd(tcourse[notnoise_index,:], axis = 1)\n",
    "\n",
    "                    for i, pid in enumerate(data.index.tolist()):\n",
    "                        if i%50 == 0:\n",
    "                            print('Working on {0} of {1} components'.format(i, len(data)))\n",
    "                        w = wave(data = tcourse[i], fps = args['fps'], mother = 'MORLET',\n",
    "                                 param = 4, siglvl = 0.99, verbose = False, plot=False)\n",
    "                        w.globalWaveletSpectrum()\n",
    "                        \n",
    "                        ratio, index = findSig()\n",
    "                        m = findContinBool(index)\n",
    "                        m = temporalCharacterize(m, ratio, w.flambda)\n",
    "                        n = sortNestedDict(m, sortkey = 'freq.rangesz')\n",
    "                        \n",
    "                        data.at[pid,'temporal.n.freq'] = len(n)\n",
    "                        if len(n) > 0:\n",
    "                            for key, val in m[n[0]].items():\n",
    "                                if key == 'freq.index':\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    data.at[pid, key] = val\n",
    "\n",
    "                    #Save file for future manipulations\n",
    "                    print('Saving dataframe to:', savepath)\n",
    "                    data.to_csv(savepath, sep = '\\t')\n",
    "\n",
    "            elif path.endswith('.tsv'):\n",
    "                try:\n",
    "                    data = pd.DataFrame.from_csv(path, sep = '\\t', index_col='exp_ic')\n",
    "                    print('Importing dataframe\\n------------------------------------')\n",
    "                    print(data.head())\n",
    "                    signal = data['signal']\n",
    "                except Exception as e:\n",
    "                    print('Error importing dataFrame')\n",
    "                    print('\\t ERROR : ', e)\n",
    "            \n",
    "            elif path.endswith('.csv'):\n",
    "                try:\n",
    "                    data = pd.DataFrame.from_csv(path, sep = ',', index_col='exp_ic')\n",
    "                    print('Importing dataframe\\n------------------------------------')\n",
    "                    print(data.head())\n",
    "                    signal = data['signal']\n",
    "                except Exception as e:\n",
    "                    print('Error importing dataFrame')\n",
    "                    print('\\t ERROR : ', e)\n",
    "            \n",
    "            else:\n",
    "                base = os.path.basename(path)\n",
    "                print('\\nFile type {} not understood.'.format(base))\n",
    "                print('skipping ' + path + '\\n')\n",
    "\n",
    "            if np.sum(signal)==0:\n",
    "                if path.endswith('.tsv'):\n",
    "                    hdf5path = path.replace('_metrics.tsv', '.hdf5')\n",
    "                    savepath = path\n",
    "\n",
    "                elif path.endswith('.hdf5'):\n",
    "                    hdf5path = path\n",
    "                    savepath = path.replace('.hdf5', '_metrics.tsv')\n",
    "\n",
    "                #put the dataframe on a scale 0 to 1 (expect age) and select only non-noise\n",
    "                datacopy = data.drop('age', axis =1).copy()\n",
    "                datacopy -= datacopy.min()\n",
    "                datacopy /= datacopy.max()\n",
    "                datacopy['age'] = data['age']\n",
    "\n",
    "                X = datacopy.fillna(value=0).loc[:, new_vars]\n",
    "\n",
    "                #load classfier hdf5\n",
    "        \n",
    "                g = h5(classifier)\n",
    "                voting_clf = g.load('voting_clf')\n",
    "                new_vars = g.load('training_keys')      \n",
    "\n",
    "                # # load through joblib\n",
    "                # voting_clf = joblib.load(classifier)\n",
    "                # new_vars = [#spetial metrics \n",
    "                        # 'spatial.std', 'spatial.min', 'spatial.max', 'mass.perc', \n",
    "                        # 'region.minaxis', 'region.majaxis', 'region.orient', \n",
    "                        # 'threshold.area', 'region.eccentricity',\n",
    "                        # #temporal metrics\n",
    "                        # 'temporal.max', 'freq.maxsnr.freq', 'length', 'freq.rangesz']\n",
    "\n",
    "                #run classifier\n",
    "                data.loc[:, 'signal'] = voting_clf.predict(X)\n",
    "\n",
    "                data['artifact'] = np.array(data['signal'] == 0).astype(int)\n",
    "                data.sort_index()\n",
    "\n",
    "                f = h5(hdf5path)\n",
    "                noise = f.load('noise_components')\n",
    "                print(noise)\n",
    "                artifact = noise.copy() * 0\n",
    "                artifact[noise==0] = data['artifact'].values.tolist()\n",
    "                print(artifact)\n",
    "                f.save({'artifact_components':artifact})\n",
    "\n",
    "                print(len(artifact))\n",
    "                print(data.head())\n",
    "\n",
    "                #Save file for future manipulations\n",
    "                print('Saving dataframe to:', savepath)\n",
    "                data.to_csv(savepath, sep = '\\t')\n",
    "\n",
    "            if args['updateDF']:\n",
    "                #open dataframe\n",
    "                try:\n",
    "                    main_data = pd.DataFrame.from_csv(classMetricsPath, sep = '\\t', index_col='exp_ic')\n",
    "                    print('Importing dataframe\\n------------------------------------')\n",
    "                    print(main_data.head())\n",
    "                except Exception as e:\n",
    "                    main_data = pd.DataFrame()\n",
    "                    print('Error importing dataFrame. Creating new main dataframe')\n",
    "                    print('\\t ERROR : ', e)\n",
    "\n",
    "                #put the dataframe on a scale 0 to 1 (expect age) and select only non-noise\n",
    "                datacopy = data.drop('age', axis =1).copy()\n",
    "                datacopy -= datacopy.min()\n",
    "                datacopy /= datacopy.max()\n",
    "                datacopy['age'] = data['age']\n",
    "\n",
    "               #update dataframe\n",
    "                main_data = main_data.combine_first(datacopy.fillna(value=0))\n",
    "                \n",
    "                #save dataframe\n",
    "                main_data.to_csv(classMetricsPath, sep = '\\t')\n",
    "\n",
    "    if args['train']:\n",
    "        if not args['updateDF']:\n",
    "            try:\n",
    "                main_data = pd.DataFrame.from_csv(classMetricsPath, sep = '\\t', index_col='exp_ic')\n",
    "                print('Importing dataframe\\n------------------------------------')\n",
    "                print(main_data.head())\n",
    "            except Exception as e:\n",
    "                main_data = pd.DataFrame()\n",
    "                print('Error importing dataFrame')\n",
    "                print('\\t ERROR : ', e)\n",
    "                assert not main_data.empty, 'Check path to metrics dataframe'\n",
    "\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "\n",
    "        new_vars = [#spetial metrics \n",
    "                        'spatial.std', 'spatial.min', 'spatial.max', 'mass.perc', \n",
    "                        'region.minaxis', 'region.majaxis', 'region.orient', \n",
    "                        'threshold.area', 'region.eccentricity',\n",
    "                        #temporal metrics\n",
    "                        'temporal.max', 'freq.maxsnr.freq', 'length', 'freq.rangesz']\n",
    "\n",
    "        y = main_data.loc[:,'signal'].values\n",
    "        X = main_data.loc[:, main_data.columns != 'signal'].fillna(value=0)\n",
    "\n",
    "        classConfidence = pd.DataFrame(index=X.index)\n",
    "        print (data)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[new_vars], y, \n",
    "            test_size=0.3, random_state=42)\n",
    "\n",
    "        #Logistic Regression\n",
    "        logreg = LogisticRegression(C  = 1, solver = 'liblinear')\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        print('Accuracy of Logistic Regression classifier on test set:\\t\\\n",
    "                {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "        classConfidence.loc[X_test.index, 'logreg_prob'] = logreg.predict_proba(X_test)[:,1]\n",
    "        classConfidence.loc[X_train.index, 'logreg_prob'] = logreg.predict_proba(X_train)[:,1]\n",
    "\n",
    "        #Gaussian Naive Bayes\n",
    "        gnb_clf = GaussianNB(var_smoothing= 0.10)\n",
    "        gnb_clf.fit(X_train, y_train)\n",
    "        y_pred = gnb_clf.predict(X_test)\n",
    "        print('Accuracy of Gaussian NB classifier on test set:\\t\\t\\\n",
    "                {:.2f}'.format(gnb_clf.score(X_test, y_test)))\n",
    "\n",
    "        classConfidence.loc[X_test.index, 'gnb_prob'] = gnb_clf.predict_proba(X_test)[:,1]\n",
    "        classConfidence.loc[X_train.index, 'gnb_prob'] = gnb_clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "        # Support Vector Machine Gaussian Kernal\n",
    "        svm_clf = SVC(kernel=\"rbf\", gamma=0.01, C=50, random_state=42, probability= True)\n",
    "        svm_clf.fit(X_train, y_train)\n",
    "        y_pred = svm_clf.predict(X_test)\n",
    "        print('Accuracy of SVM classifier on test set:\\t\\t\\t\\\n",
    "                {:.2f}'.format(svm_clf.score(X_test, y_test)))\n",
    "\n",
    "        classConfidence.loc[X_test.index, 'SVC_prob'] = svm_clf.predict_proba(X_test)[:,1]\n",
    "        classConfidence.loc[X_train.index, 'SVC_prob'] = svm_clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "        #Random Forest Classifier\n",
    "        rnd_clf = RandomForestClassifier(n_estimators = 500, max_features = len(new_vars), random_state=42)\n",
    "        rnd_clf.fit(X_train, y_train)\n",
    "        y_pred = rnd_clf.predict(X_test)\n",
    "        print('Accuracy of Random Forest classifier on test set:\\t\\\n",
    "                {:.2f}'.format(rnd_clf.score(X_test, y_test)))\n",
    "        classConfidence.loc[X_test.index, 'rnd_clf_prob'] = rnd_clf.predict_proba(X_test)[:,1]\n",
    "        classConfidence.loc[X_train.index, 'rnd_clf_prob'] = rnd_clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "        #Voting Classifier\n",
    "        voting_clf = VotingClassifier(\n",
    "            estimators=[#('lr', logreg), \n",
    "                        #('gnb', gnb_clf), \n",
    "                        ('rf', rnd_clf), \n",
    "                        ('svc', svm_clf)], voting='soft')\n",
    "        voting_clf.fit(X_train, y_train)\n",
    "        y_pred = voting_clf.predict(X_test)\n",
    "        print('Accuracy of Voting classifier on test set:\\t\\t\\\n",
    "                {:.2f}'.format(voting_clf.score(X_test, y_test)))\n",
    "        classConfidence.loc[X_test.index, 'voting_clf_prob'] = voting_clf.predict_proba(X_test)[:,1]\n",
    "        classConfidence.loc[X_train.index, 'voting_clf_prob'] = voting_clf.predict_proba(X_train)[:,1]\n",
    "        \n",
    "        g = h5(classifier)\n",
    "        g.save({'voting_clf':voting_clf, 'training_keys':new_vars })        \n",
    "\n",
    "        if args['plot']:\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            from sklearn.metrics import roc_curve\n",
    "            \n",
    "            classConfidence = 2*(classConfidence - 0.5)\n",
    "            classConfidence['x'] = np.arange(len(classConfidence))\n",
    "            start_exp = []\n",
    "            end_exp = []\n",
    "            for j, i in enumerate(classConfidence.index.tolist()):\n",
    "                domain = int(i[-4:])\n",
    "                if domain == 0:\n",
    "                    start_exp.append(j)\n",
    "\n",
    "            for i in start_exp:\n",
    "                if i != 0:\n",
    "                    end_exp.append(i-1)\n",
    "            end_exp.append(len(classConfidence))\n",
    "\n",
    "            alphas = [0.25, 0.25, 0.25, 0.25, 1] \n",
    "            colors = sns.color_palette()\n",
    "            clfs = ['logreg_prob', 'gnb_prob', 'SVC_prob', 'rnd_clf_prob', 'voting_clf_prob']\n",
    "            \n",
    "            classConfidence.loc[X_train.index,'predicted'] = voting_clf.predict(X_train)\n",
    "            classConfidence.loc[X_test.index,'predicted'] = voting_clf.predict(X_test)\n",
    "            classConfidence.sort_index()\n",
    "            classConfidence['marked'] = y\n",
    "            classConfidence['false'] = classConfidence['predicted'] - classConfidence['marked']\n",
    "\n",
    "            for i, clf in enumerate(clfs):\n",
    "                if i == 0:\n",
    "                    ax1 = classConfidence.plot(kind = 'scatter', x = 'x', y= clf, label = clf,\n",
    "                                        color = colors[i], alpha = alphas[i], figsize=(20,4), grid=False)\n",
    "                else:\n",
    "                    classConfidence.plot(kind = 'scatter', x = 'x', y= clf, label = clf,\n",
    "                                         color = colors[i], alpha = alphas[i], grid=False, ax = ax1)\n",
    "            plt.axhline(y = 0, color = 'k', linestyle = '--')\n",
    "            plt.ylabel('artifact                                  signal')\n",
    "\n",
    "            xlabel = []\n",
    "            falsePos = True\n",
    "            falseNeg = True\n",
    "            linemax = 0.94\n",
    "\n",
    "            for j, i in enumerate(classConfidence['false'].tolist()):\n",
    "                if i < 0:\n",
    "                    if falseNeg:\n",
    "                        plt.axvline(x=j, ymax = linemax, color = 'r', alpha=0.5, label = 'False Negative')\n",
    "                        falseNeg = False\n",
    "                    else:\n",
    "                        plt.axvline(x=j, ymax = linemax, color = 'r', alpha=0.5)\n",
    "                elif i > 0:\n",
    "                    if falsePos:\n",
    "                        plt.axvline(x=j, ymax = linemax, color = 'b', alpha=0.5, label = 'False Positive')\n",
    "                        falsePos = False\n",
    "                    else:\n",
    "                        plt.axvline(x=j, ymax = linemax, color = 'b', alpha=0.5)\n",
    "\n",
    "            for j, i in enumerate(start_exp):\n",
    "                if (j%2==0):\n",
    "                    plt.axvspan(xmin=i, xmax =end_exp[j]+1, color='k', alpha=0.1)\n",
    "                plt.text(i, 1.05, classConfidence.index[i])\n",
    "            \n",
    "            # plt.xticks(start_exp, xlabel)\n",
    "            leg = plt.legend(title = 'Types of Classifier', bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "            leg.get_frame().set_linewidth(0.0)\n",
    "            plt.xlabel('Num of Domains')\n",
    "            plt.ylim([-1.05,1.15])           \n",
    "            plt.show()\n",
    "\n",
    "            plt.figure()\n",
    "\n",
    "            logreg.fit(X_train, y_train)\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "            plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "\n",
    "            gnb_clf.fit(X_train, y_train)\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test, gnb_clf.predict(X_test))\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, gnb_clf.predict_proba(X_test)[:,1])\n",
    "            plt.plot(fpr, tpr, label='GaussianNB (area = %0.2f)' % roc_auc)\n",
    "\n",
    "            svm_clf.fit(X_train, y_train)\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test, svm_clf.predict(X_test))\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, svm_clf.predict_proba(X_test)[:,1])\n",
    "            plt.plot(fpr, tpr, label='SVC (area = %0.2f)' % roc_auc)\n",
    "\n",
    "            rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test, rnd_clf.predict(X_test))\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, rnd_clf.predict_proba(X_test)[:,1])\n",
    "            plt.plot(fpr, tpr, label='Random Forest Classifier (area = %0.2f)' % roc_auc)\n",
    "\n",
    "            voting_clf = VotingClassifier(\n",
    "                estimators=[#('lr', logreg), \n",
    "                            #('gnb', gnb_clf), \n",
    "                            ('rf', rnd_clf), \n",
    "                            ('svc', svm_clf)], voting='soft')\n",
    "            voting_clf.fit(X_train, y_train)\n",
    "\n",
    "            roc_auc = roc_auc_score(y_test, voting_clf.predict(X_test))\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, voting_clf.predict_proba(X_test)[:,1])\n",
    "            plt.plot(fpr, tpr, label='Voting Classifier (area = %0.2f)' % roc_auc)\n",
    "\n",
    "            plt.plot([0, 1], [0, 1],'r--')\n",
    "            plt.xlim([-0.025, 1.0])\n",
    "            plt.ylim([0, 1.025])\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title('Receiver operating characteristic')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            # plt.savefig('rnd_ROC.svg')\n",
    "            plt.show()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
